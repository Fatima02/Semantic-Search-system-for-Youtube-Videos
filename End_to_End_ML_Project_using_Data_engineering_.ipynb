{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "py9o8Hxx1tYl",
        "outputId": "b5b27e2a-3199-40f0-8e4e-1465b7023f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jupyterlab==4.1.6 (from -r requirements.txt (line 1))\n",
            "  Downloading jupyterlab-4.1.6-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting polars==0.20.21 (from -r requirements.txt (line 2))\n",
            "  Downloading polars-0.20.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting youtube_transcript_api==0.6.2 (from -r requirements.txt (line 3))\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting matplotlib==3.8.0 (from -r requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting sentence-transformers==2.6.1 (from -r requirements.txt (line 6))\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting scikit-learn==1.2.2 (from -r requirements.txt (line 7))\n",
            "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.26.4)\n",
            "Collecting gradio==4.29.0 (from -r requirements.txt (line 9))\n",
            "  Downloading gradio-4.29.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting httpx>=0.25.0 (from jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting ipykernel>=6.5.0 (from jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from jupyterlab==4.1.6->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from jupyterlab==4.1.6->-r requirements.txt (line 1)) (5.7.2)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.19.0 (from jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.2.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from jupyterlab==4.1.6->-r requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: tomli>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab==4.1.6->-r requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab==4.1.6->-r requirements.txt (line 1)) (6.3.3)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.10/dist-packages (from jupyterlab==4.1.6->-r requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube_transcript_api==0.6.2->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 4)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 4)) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 4)) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.6.1->-r requirements.txt (line 6)) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.6.1->-r requirements.txt (line 6)) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.6.1->-r requirements.txt (line 6)) (2.4.1+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.6.1->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.6.1->-r requirements.txt (line 6)) (0.24.7)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (3.5.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (4.2.2)\n",
            "Collecting fastapi (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==0.16.1 (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading gradio_client-0.16.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (6.4.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (2.1.4)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (2.9.2)\n",
            "Collecting pydub (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading python_multipart-0.0.11-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==4.29.0->-r requirements.txt (line 9)) (2.2.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.1->gradio==4.29.0->-r requirements.txt (line 9)) (2024.6.1)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.16.1->gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 9)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 9)) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 9)) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.6.1->-r requirements.txt (line 6)) (3.16.1)\n",
            "Collecting comm>=0.1.1 (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.6.6)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (24.0.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->jupyterlab==4.1.6->-r requirements.txt (line 1)) (4.3.6)\n",
            "Requirement already satisfied: argon2-cffi>=21.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (23.1.0)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-events>=0.9.0 (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_events-0.10.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (6.5.4)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (5.10.4)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: prometheus-client>=0.9 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.21.0)\n",
            "Requirement already satisfied: send2trash>=1.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.18.1)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (2.16.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.19.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading json5-0.9.25-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.29.0->-r requirements.txt (line 9)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==4.29.0->-r requirements.txt (line 9)) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.29.0->-r requirements.txt (line 9)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio==4.29.0->-r requirements.txt (line 9)) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.0->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube_transcript_api==0.6.2->-r requirements.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1->-r requirements.txt (line 6)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==2.6.1->-r requirements.txt (line 6)) (3.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1->-r requirements.txt (line 6)) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1->-r requirements.txt (line 6)) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers==2.6.1->-r requirements.txt (line 6)) (0.19.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 9)) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 9)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 9)) (13.8.1)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi->gradio==4.29.0->-r requirements.txt (line 9))\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (21.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 9)) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 9)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 9)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.29.0->-r requirements.txt (line 9)) (0.20.0)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading python_json_logger-2.0.7-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (2.20.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 9)) (3.0.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers==2.6.1->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.8.4)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (24.8.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.29.0->-r requirements.txt (line 9)) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.2.13)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1)) (2.22)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab==4.1.6->-r requirements.txt (line 1))\n",
            "  Downloading types_python_dateutil-2.9.0.20240906-py3-none-any.whl.metadata (1.9 kB)\n",
            "Downloading jupyterlab-4.1.6-py3-none-any.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polars-0.20.21-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.29.0-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.16.1-py3-none-any.whl (314 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.6/314.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.14.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.11-py3-none-any.whl (22 kB)\n",
            "Downloading ruff-0.6.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m620.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.0-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.9.25-py3-none-any.whl (30 kB)\n",
            "Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_events-0.10.0-py3-none-any.whl (18 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20240906-py3-none-any.whl (9.7 kB)\n",
            "Installing collected packages: pydub, websockets, uri-template, types-python-dateutil, tomlkit, semantic-version, ruff, rfc3986-validator, rfc3339-validator, python-multipart, python-json-logger, polars, overrides, orjson, jsonpointer, json5, jedi, h11, fqdn, ffmpy, comm, async-lru, aiofiles, youtube_transcript_api, uvicorn, starlette, scikit-learn, matplotlib, jupyter-server-terminals, jupyter-client, httpcore, arrow, isoduration, ipykernel, httpx, fastapi, gradio-client, sentence-transformers, jupyter-events, gradio, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab\n",
            "  Attempting uninstall: polars\n",
            "    Found existing installation: polars 1.6.0\n",
            "    Uninstalling polars-1.6.0:\n",
            "      Successfully uninstalled polars-1.6.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\n",
            "notebook 6.5.5 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 arrow-1.3.0 async-lru-2.0.4 comm-0.2.2 fastapi-0.115.0 ffmpy-0.4.0 fqdn-1.5.1 gradio-4.29.0 gradio-client-0.16.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 ipykernel-6.29.5 isoduration-20.11.0 jedi-0.19.1 json5-0.9.25 jsonpointer-3.0.0 jupyter-client-8.6.3 jupyter-events-0.10.0 jupyter-lsp-2.2.5 jupyter-server-2.14.2 jupyter-server-terminals-0.5.3 jupyterlab-4.1.6 jupyterlab-server-2.27.3 matplotlib-3.8.0 orjson-3.10.7 overrides-7.7.0 polars-0.20.21 pydub-0.25.1 python-json-logger-2.0.7 python-multipart-0.0.11 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 ruff-0.6.8 scikit-learn-1.2.2 semantic-version-2.10.0 sentence-transformers-2.6.1 starlette-0.38.6 tomlkit-0.12.0 types-python-dateutil-2.9.0.20240906 uri-template-1.3.0 uvicorn-0.31.0 websockets-11.0.3 youtube_transcript_api-0.6.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "452946294dbf427fad458c7b30c891f7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_key = 'AIzaSyAN2QTXP-OIzjQ7ejp6EKxyc63MQGNOltY'"
      ],
      "metadata": {
        "id": "F1S5qtph29xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import polars as pl\n"
      ],
      "metadata": {
        "id": "nBxLJ3tt3nAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getVideoRecords(response: requests.models.Response) -> list:\n",
        "    \"\"\"\n",
        "        Function to extract YouTube video data from GET request response\n",
        "    \"\"\"\n",
        "\n",
        "    video_record_list = []\n",
        "\n",
        "    for raw_item in json.loads(response.text)['items']:\n",
        "\n",
        "        # only execute for youtube videos\n",
        "        if raw_item['id']['kind'] != \"youtube#video\":\n",
        "            continue\n",
        "\n",
        "        video_record = {}\n",
        "        video_record['video_id'] = raw_item['id']['videoId']\n",
        "        video_record['datetime'] = raw_item['snippet']['publishedAt']\n",
        "        video_record['title'] = raw_item['snippet']['title']\n",
        "\n",
        "        video_record_list.append(video_record)\n",
        "\n",
        "    return video_record_list"
      ],
      "metadata": {
        "id": "2DMrFelX3-n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define channel ID\n",
        "channel_id = 'UCa9gErQ9AE5jT2DZLjXBIdA'\n",
        "\n",
        "# define url for API\n",
        "url = 'https://www.googleapis.com/youtube/v3/search'\n",
        "\n",
        "# initialize page token\n",
        "page_token = None\n",
        "\n",
        "# intialize list to store video data\n",
        "video_record_list = []"
      ],
      "metadata": {
        "id": "gMCX-ZUy4D0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# extract video data across multiple search result pages\n",
        "while page_token != 0:\n",
        "    # define parameters for API call\n",
        "    params = {\"key\": my_key, 'channelId': channel_id, 'part': [\"snippet\",\"id\"], 'order': \"date\", 'maxResults':50, 'pageToken': page_token}\n",
        "    # make get request\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    # append video records to list\n",
        "    video_record_list += getVideoRecords(response)\n",
        "\n",
        "    try:\n",
        "        # grab next page token\n",
        "        page_token = json.loads(response.text)['nextPageToken']\n",
        "    except:\n",
        "        # if no next page token kill while loop\n",
        "        page_token = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgdm2ATe41V3",
        "outputId": "65e8feb5-e33e-43c5-8445-71522f8d053e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 46.4 ms, sys: 3.99 ms, total: 50.4 ms\n",
            "Wall time: 938 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write data to file\n",
        "pl.DataFrame(video_record_list).write_parquet('/content/video-ids.parquet')\n",
        "pl.DataFrame(video_record_list).write_csv('/content/video-ids.csv')"
      ],
      "metadata": {
        "id": "HhV19PFl49kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming video_record_list is your data\n",
        "df = pl.DataFrame(video_record_list)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcfdyTsH6Qb5",
        "outputId": "4825bfb9-541d-4e68-f78c-541855d19f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (106, 3)\n",
            "┌─────────────┬──────────────────────┬───────────────────────────────────┐\n",
            "│ video_id    ┆ datetime             ┆ title                             │\n",
            "│ ---         ┆ ---                  ┆ ---                               │\n",
            "│ str         ┆ str                  ┆ str                               │\n",
            "╞═════════════╪══════════════════════╪═══════════════════════════════════╡\n",
            "│ 7Oy2NmPwJXo ┆ 2024-09-26T23:24:35Z ┆ I Quit My Job… Here’s How Much I… │\n",
            "│ ZVVkdXHqEuM ┆ 2024-09-23T15:45:12Z ┆ Knowledge Distillation Explained… │\n",
            "│ reXoKNC_Wx4 ┆ 2024-09-20T18:15:44Z ┆ Quantization Explained in 60 Sec… │\n",
            "│ 9joIFeKuf04 ┆ 2024-09-16T14:01:44Z ┆ Python Explained in 60 Seconds #… │\n",
            "│ pNg2DJ4spXg ┆ 2024-09-12T16:36:12Z ┆ [Mini-Course] Python QuickStart … │\n",
            "│ …           ┆ …                    ┆ …                                 │\n",
            "│ MX7ymkYGiZ0 ┆ 2020-12-21T00:24:45Z ┆ The Wavelet Transform | Introduc… │\n",
            "│ rPUytg38b6Q ┆ 2020-12-04T01:10:36Z ┆ The Fast Fourier Transform | How… │\n",
            "│ mj86XmfOniY ┆ 2020-11-15T20:41:53Z ┆ Time Series, Signals, &amp; the … │\n",
            "│ Gwz4zXPeP_Q ┆ 2020-11-12T22:58:00Z ┆ biometricDahboard3 DEMO           │\n",
            "│ lciC1s4FO0g ┆ 2020-09-23T13:02:57Z ┆ biometricDashboard2 DEMO          │\n",
            "└─────────────┴──────────────────────┴───────────────────────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Automatic Transcripts from YouTube Videos**\n"
      ],
      "metadata": {
        "id": "1gN7KFfM6Sn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ],
      "metadata": {
        "id": "MQ0J7P9f6Yq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(transcript: list) -> str:\n",
        "    \"\"\"\n",
        "        Function to extract text from transcript dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    text_list = [transcript[i]['text'] for i in range(len(transcript))]\n",
        "    return ' '.join(text_list)"
      ],
      "metadata": {
        "id": "-gBMdX536gy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get transcripts\n",
        "df = pl.read_parquet('/content/video-ids.parquet')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys5luaZt61Ju",
        "outputId": "e5299730-311e-488e-9c8d-8a2b1b086851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 3)\n",
            "┌─────────────┬──────────────────────┬───────────────────────────────────┐\n",
            "│ video_id    ┆ datetime             ┆ title                             │\n",
            "│ ---         ┆ ---                  ┆ ---                               │\n",
            "│ str         ┆ str                  ┆ str                               │\n",
            "╞═════════════╪══════════════════════╪═══════════════════════════════════╡\n",
            "│ 7Oy2NmPwJXo ┆ 2024-09-26T23:24:35Z ┆ I Quit My Job… Here’s How Much I… │\n",
            "│ ZVVkdXHqEuM ┆ 2024-09-23T15:45:12Z ┆ Knowledge Distillation Explained… │\n",
            "│ reXoKNC_Wx4 ┆ 2024-09-20T18:15:44Z ┆ Quantization Explained in 60 Sec… │\n",
            "│ 9joIFeKuf04 ┆ 2024-09-16T14:01:44Z ┆ Python Explained in 60 Seconds #… │\n",
            "│ pNg2DJ4spXg ┆ 2024-09-12T16:36:12Z ┆ [Mini-Course] Python QuickStart … │\n",
            "└─────────────┴──────────────────────┴───────────────────────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "transcript_text_list = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "\n",
        "    # try to extract captions\n",
        "    try:\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(df['video_id'][i])\n",
        "        transcript_text = extract_text(transcript)\n",
        "    # if not available set as n/a\n",
        "    except:\n",
        "        transcript_text = \"n/a\"\n",
        "\n",
        "    transcript_text_list.append(transcript_text)"
      ],
      "metadata": {
        "id": "v7lhfrUO7DM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add transcripts to dataframe\n",
        "df = df.with_columns(pl.Series(name=\"transcript\", values=transcript_text_list))\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2tnBYdz7C8P",
        "outputId": "e41e6742-74eb-4f96-ea94-e25e2c42d9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 4)\n",
            "┌─────────────┬──────────────────────┬──────────────────────────────┬──────────────────────────────┐\n",
            "│ video_id    ┆ datetime             ┆ title                        ┆ transcript                   │\n",
            "│ ---         ┆ ---                  ┆ ---                          ┆ ---                          │\n",
            "│ str         ┆ str                  ┆ str                          ┆ str                          │\n",
            "╞═════════════╪══════════════════════╪══════════════════════════════╪══════════════════════════════╡\n",
            "│ 7Oy2NmPwJXo ┆ 2024-09-26T23:24:35Z ┆ I Quit My Job… Here’s How    ┆ 14 months ago I made a big   │\n",
            "│             ┆                      ┆ Much I…                      ┆ life …                       │\n",
            "│ ZVVkdXHqEuM ┆ 2024-09-23T15:45:12Z ┆ Knowledge Distillation       ┆ knowledge distillation       │\n",
            "│             ┆                      ┆ Explained…                   ┆ explained…                   │\n",
            "│ reXoKNC_Wx4 ┆ 2024-09-20T18:15:44Z ┆ Quantization Explained in 60 ┆ here's quantization          │\n",
            "│             ┆                      ┆ Sec…                         ┆ explained in…                │\n",
            "│ 9joIFeKuf04 ┆ 2024-09-16T14:01:44Z ┆ Python Explained in 60       ┆ here's python explained in   │\n",
            "│             ┆                      ┆ Seconds #…                   ┆ 60 se…                       │\n",
            "│ pNg2DJ4spXg ┆ 2024-09-12T16:36:12Z ┆ [Mini-Course] Python         ┆ python is the go-to          │\n",
            "│             ┆                      ┆ QuickStart …                 ┆ programming …                │\n",
            "└─────────────┴──────────────────────┴──────────────────────────────┴──────────────────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write data to file\n",
        "df.write_parquet('/content/video-transcripts.parquet')\n",
        "#df.write_csv('data/video-transcripts.csv')"
      ],
      "metadata": {
        "id": "cMA_SUpN9BP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validate Video Data**"
      ],
      "metadata": {
        "id": "IT1IBdkI9TLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "AfHYk6dt9R8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pl.read_parquet('/content/video-transcripts.parquet')"
      ],
      "metadata": {
        "id": "5xiw5vhe9ip7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape + unique values\n",
        "print(\"shape:\", df.shape)\n",
        "print(\"n unique rows:\", df.n_unique())\n",
        "for j in range(df.shape[1]):\n",
        "    print(\"n unique elements (\" + df.columns[j] + \"):\", df[:,j].n_unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWnQ_U6N9i9Y",
        "outputId": "eacf8ee3-d3b6-402c-8bc4-17b79b577e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (106, 4)\n",
            "n unique rows: 106\n",
            "n unique elements (video_id): 106\n",
            "n unique elements (datetime): 106\n",
            "n unique elements (title): 106\n",
            "n unique elements (transcript): 104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of title characters:\", sum(len(df['title'][i]) for i in range(len(df))))\n",
        "print(\"Total number of transcript characters:\", sum(len(df['transcript'][i]) for i in range(len(df))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnODRkCM9xSh",
        "outputId": "c59cc365-6e5e-4876-ec78-10582f1b853e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of title characters: 5559\n",
            "Total number of transcript characters: 1188607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change datetime to Datetime dtype\n",
        "df = df.with_columns(pl.col('datetime').cast(pl.Datetime))\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd0IlGM291iS",
        "outputId": "a629945f-0023-4d76-9d34-0f995cac5693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 4)\n",
            "┌─────────────┬─────────────────────┬───────────────────────────────┬──────────────────────────────┐\n",
            "│ video_id    ┆ datetime            ┆ title                         ┆ transcript                   │\n",
            "│ ---         ┆ ---                 ┆ ---                           ┆ ---                          │\n",
            "│ str         ┆ datetime[μs]        ┆ str                           ┆ str                          │\n",
            "╞═════════════╪═════════════════════╪═══════════════════════════════╪══════════════════════════════╡\n",
            "│ 7Oy2NmPwJXo ┆ 2024-09-26 23:24:35 ┆ I Quit My Job… Here’s How     ┆ 14 months ago I made a big   │\n",
            "│             ┆                     ┆ Much I…                       ┆ life …                       │\n",
            "│ ZVVkdXHqEuM ┆ 2024-09-23 15:45:12 ┆ Knowledge Distillation        ┆ knowledge distillation       │\n",
            "│             ┆                     ┆ Explained…                    ┆ explained…                   │\n",
            "│ reXoKNC_Wx4 ┆ 2024-09-20 18:15:44 ┆ Quantization Explained in 60  ┆ here's quantization          │\n",
            "│             ┆                     ┆ Sec…                          ┆ explained in…                │\n",
            "│ 9joIFeKuf04 ┆ 2024-09-16 14:01:44 ┆ Python Explained in 60        ┆ here's python explained in   │\n",
            "│             ┆                     ┆ Seconds #…                    ┆ 60 se…                       │\n",
            "│ pNg2DJ4spXg ┆ 2024-09-12 16:36:12 ┆ [Mini-Course] Python          ┆ python is the go-to          │\n",
            "│             ┆                     ┆ QuickStart …                  ┆ programming …                │\n",
            "└─────────────┴─────────────────────┴───────────────────────────────┴──────────────────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lengths/character counts\n",
        "plt.hist(df['title'].str.len_chars())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "rWwmbgmm9-ol",
        "outputId": "3e3c0e4c-5e8a-4ca6-cf26-ac22cf8113f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 3.,  5., 16., 18., 14., 24., 14.,  9.,  1.,  2.]),\n",
              " array([16. , 23.8, 31.6, 39.4, 47.2, 55. , 62.8, 70.6, 78.4, 86.2, 94. ]),\n",
              " <BarContainer object of 10 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#handling special strings\n",
        "print(df['title'][9])\n",
        "print(df['transcript'][9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRTN71_j-DfI",
        "outputId": "02835055-9a0c-4c9e-9b8c-4d5923a79252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM Fine-tuning on Mac (M1 16GB)\n",
            "with the rise of open- source models and efficient fine-tuning methods it's never been easier to build custom ml solutions for example anyone with a single GPU can now fine-tune a large language model on their local machine which is exactly what I did in a previous video of this series however since my machine is an M series Mac which doesn't have an Nvidia GPU I had to use the free GPU on Google collab to run that example this is somewhat disappoint pointing because using collabs free gpus is somewhat restrictive and not as convenient as running something on my local machine that's why in this video I'm going to share an easy way to fine-tune an llm locally on Mac and if you're new here welcome I'm sha I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make if you've been keeping up with machine learning over the past decade then you're probably familiar with Nvidia and all their different gpus it turns out that gpus are super helpful for machine learning because they can much more efficiently train and run machine learning models than traditional CPUs they've also demonstrated an ability to take Nvidia from a hundred billion valuation all the way up to 3 trillion in the past 5 years nvidia's dominance of the GPU Market has greatly influenced the available open-source tools for running and training neural networks the result of this is that a lot of open- source tools work seamlessly with Nvidia Hardware while this is great for Windows and Linux users it often leaves Mac users like me left sitting on these Sidelines after a failed attempt to locally fine-tune llama 2 on my local machine my impression was pulling this off is something that would take several hours of effort that was until I discovered the mlx python library mlx is a python Library developed by Apple's machine learning research team for efficiently running Matrix operations on Apple silicon so we see the documentation here it's inspired by Frameworks like pie torch Jacks and array file one of the notable differences with mlx is that it allows you to use the unified memory model of these M1 chips no longer do you have to worry about RAM and vram being separate things M1 chips just have a single memory so that means me with my Mac Mini M1 2020 with only 16 gigs of memory am capable of fine-tuning a large language model locally on my machine while mlx is a pretty low-level framework it's not going to have highlevel abstractions for loading and training models like hugging face for example there is this example implementation of Laura which is very readily hackable and adaptable to another use case which is exactly what I'm going to do here similar to the previous Cur video here I'm going to fine-tune a quantized version of mistol 7B instruct to respond to YouTube comments in my likeness however instead of using hugging face in Google collab here I'm going to use the mlx library and my local machine and again here are the specs of my machine it's a Mac Mini M1 from 2020 with only 16 GB of memory so by today's standards my machine is hilarious but despite that it's still good enough to implement ment this fine-tuning example so I've put together some example code and that's available on GitHub if you go to my YouTube blog repo go to the llm tab here we see all the different code and videos and blogs of this series so now there's a new one called kulur mlx so we click on that what I've done here is I have this notebook that walks through the example code and then I've taken all the scripts from that mlx example implementation so I've put them in the scripts folder and then here I've prepared data so this is data of my YouTube comments and I prepared it in this Json L format but we won't talk about this right now talk about it a little later okay so the first step in running this example is to go to the repo and clone it so I'll copy this Ur here I'll go over to my terminal let me zoom in a bit okay so we're going to clone the repo get clone might take a while unfortunately there's not a way to clone a specific subfolder of a GitHub repo if you you want to download some code from a repo you have to clone the entire thing go to the repo we just cloned and then the code is in the llms subdirectory and then it's Q mlx so here we see everything we've got our example code requirements file some scripts some data and a readme okay so I cloned the repo and I navigated to this folder so now let's create a python environment that allows us to run the code so here I'll create a new virtual environment called mlx DV and then I'll activate this environment so this how you do it in bash and zsh and I guess every Mac User uses this so this is how you'll activate the environment all right so now you can see we're in this mlx DV environment but the only thing in here is PIP so you see you do pip list all there is is PIP so let's install all the required libraries so we can just do the PIP install D requirements. text and this will install everything while that's happen happening we can look at what is in that so here we have mlx we have mlx LM so this is a library built on top of mlx specifically for large language models also we have the Transformers library and numpy and then finally since I wrote the code in a Jupiter notebook we'll install Jupiter lab in IPI widgets okay so we installed all the requirements and if you run into trouble in the installation steps so here are some important notes from mlx is documentation first and foremost you need the M series chip like that's the whole point of this video and this Library the second is that you are using a native python that's greater than or equal to 3.8 so I think here I'm using 3.12 if I just do this yeah so I'm using 3.2.2 then you have to have at least Mac OS 13.5 but they recommend that you have Mac OS 14 which is what I'm running on my machine right now so now that we've cloned the repo and we set up our environment let's run through the example code so to do that we'll do Jupiter lab okay so here we've got the example code video link and blog link coming soon cuz I'm making it right now and then we're going to zoom in because it's probably tiny on your screen all right so mlx fine tuning we're going to do some imports so we're going to import subprocess because all the example code that we're hacking runs from the command line and so subprocess allows us to run terminal commands through Python and then here I'm importing the mlx LM library to run inference on a model a little later so I defined some helper functions here they're not super important right now so I'll just come back to them as we encounter them in the code and then this is an optional step that example code from mlx comes with this convert. py script which is capable of taking any model from the hugging face Hub and converting it into the proper mlx format and additionally quantizing it so I actually did this for mistol 7B instruct version 0.2 and then there there's this argument that you can pass that will push the model to the mlx community so I guess that's worth calling out right now mlx has this page on hugging face here are several mlx compatible models many of which are quantized so there's Google Gemma mistol quen code Gemma llama 3 53 whisper llama 3.1 even so this page seems pretty active and basically any model that you would want to find tune is probably already available here and if it's not you can simply just go find the model that you want to use in mlx let's go to mistal AI I'll go to Mistral 7B instruct so this is the one I used given this hugging face model path we can convert it to the mlx format and quantize it using this convert. py script so there was this typo so I had to add this quantize flag so what this will do is grab the hugging face model convert to the mlx format and this quantize flag converts into 4bit quanti ization so I actually didn't run the command yet so you could run this command using the subprocess module but I found it's better to just print the command like this and copy paste it into the terminal because when you run these shell commands in a jupyter notebook you don't see the same like progress metrics that you would normally and this will actually save a mlx version of that model on your local machine that you can run for inference and fine-tuning since here we're going to use a model that's already available on the hugging face hub here's the model card for it we can just skip this quantized model step and so here what's happening is I'm going to build the prompt that is defined here so again here we're creating a YouTube comment responder what this model will do is you'll pass in a comment and it'll respond to that comment hopefully in my likeness so for this example I'm not just going to pass the raw comment into the model itself for inference I have actually constructed this prompt which is the same prompt that we saw in the previous Cur example to help the model generate better responses I created a Lambda function for this to find this instruction string and then did some string manipulation to incorporate the comment into it dynamically the result of that is this prompt Builder Lambda function which takes in the comment and so here we're just doing a very boring comment great content thank you and then we'll Define the max number of tokens and then here we're going to use the mlx LM library to do inference so the syntax looks very similar to hugging face which is pretty convenient if you're comfortable with hugging face already so here what we're going to do is we're going to load in the model so this is that same quantize model and then we're going to have the model generate a response so we pass in the model the tokenizer which was loaded automatically The Prompt that we defined using our prompt Builder the max number of tokens which I set to 140 and then I put verbos equal to True okay so we can take a look at the prompt here which is all this stuff I'm not going to read all this you can read it if you like but this is just to help nudge the model in the right direction then we have this please respond to the following comment and this is the comment that we spliced in using that prompt Builder Lambda function and then down here is the model's response so this is the raw quantized model no fine-tuning whatsoever and this is the response well first and foremost it put sha GPT in the wrong place this is supposed to be at the end of the response not at the beginning but it says thank you for your kind words I'm glad you found the content helpful and enjoyable if you any specific questions or topics you'd like me to cover in more detail please feel free to ask I would never respond to a comment like this this is something we've seen in the previous fine-tuning videos where the unfin tuned responses tend to be pretty verbose and when I respond to comments or really any kind of communication I try to keep it as concise as possible so this is very different than something I'd actually write in response to a comment so let's see how we can fine-tune this model to generate responses that sound a lot more like me here again we're going to use one of the scripts from that mlx examples repo and I'm going to construct the command in the same way so I put everything in a list and the reason I have it in a list is because that's how you can run these terminal commands in Python you'll pass it in as a list to the subprocess module but I realized that even with this fancy helper function that chat GPT wrote me to continuously print outputs from my terminal command it still wasn't printing the loss during training so I actually found an alternative strategy to be more helpful which is to just take this command variable here and to translate it into a string that I can copy and paste into my terminal and so this construct shell command is just a pretty simple helper function up here that's just doing some basic string manipulation to convert this list into a string what I'll do is I'll copy this string and then we can go over to terminal and then I'll paste the command here here okay so walking through this a little bit I'll try to zoom in even more hopefully that's legible we're running a python script so that's what this is the python script we're running is called lowra and it's in this scripts subdirectory and then we're just going to define the parameters for training so here we're specifying the model which is that 4-bit version of mistal 7B instruct we have this train flag because we're going to run training we're going to set the number of iterations so we're going to do 100 iterations that means it's going to run through 100 batches and the default batch is four next is steps per eval so this is the number of batches that the training will run through before Computing the validation loss so I set it to 10 which is the same as the number of steps per training loss evaluation the Val batches is the number of examples to include in the validation loss calculation setting it to ne1 goes through everything in the validation data set and here that's only 10 examples next we set the learning rate at 1 to the minus 5 which is actually the same as the default but I have it explicitly written here cuz I was playing around with this I probably ran this a dozen different times trying to find the best hyper parameters and then low R layers 16 which is also the default parameter but I went through a lot of iterations to just end up coming back to the default and then finally we use this test flag which computes the test loss at the end of training I guess before we run this it's worth talking about the data that we're using here we have three data sets so we have have this train. Json L test. Json L and valid Json L the way I make these data sets are here in this jupyter notebook also available on the GitHub what I'm doing here is I'm taking a CSV file of YouTube comments and responses which looks like this so I've got 70 comments these are real comments and 70 real responses from me and so this is the way I'm going to train the model to respond to comments in my likeness and way I do the train test validation split is I have 50 examples for training 10 examples for testing and 10 examples for validation I won't walk through this code cuz I did it in the Cur video and I did a similar thing for the open AI fine-tuning API video but if you're curious about how I'm doing the data preparation feel free to check this out we're using the same prompt in the training data as I used at inference in the example code we just saw very similar strategy the Json l format if you're familiar with python is essentially a list of dictionaries so a dictionary is a set of key value Pairs and a list is just a sequence a collection of elements Json L is just going to be a collection of these key value pairs where each key value pair each dictionary consists of one key and one value so super simple the key is text and the value is the example that you want to use to train or evaluate the language model notice that this contains the instructions essentially of the model this bit is the comment the real comment from the user and then it ends with this instruction token and then here's the real response from me so all of these are packed together to form the example and we have 70 of these and 50 are going toward training 10 are going for testing and 10 are going for validation so here I randomly select examples for testing and validation and then I just write everything to these Json L files so hopefully this example code is easy for you to follow and hack and adapt for your own use case and if you get stuck you know feel free to drop a comment or reach out I'm happy to help try to get you unstuck okay so that brief aside was the data preparation now let's go into training so I actually haven't done this while running OBS so my computer might blow up if I do this so I've got activity monitor and it's not looking great because OBS is already using a gig of memory and when I was running this the fine-tuning script was taking like 14 G gabt of memory and so I'm going to execute this script but if my computer blows up and I'm not able to post this video I'm so sorry all right here goes nothing nothing has blown up yet we'll just keep the activity monitor here so we can monitor the memory pressure so we see that the fine-tuning script is taking about 10 gbes of memory there's this other python script which I guess is the Jupiter notebook taking up 4 GB of memory M and then we got OBS here taking up 1 gab of memory so when I was doing this for real I was basically doing nothing else on my machine I was just allowing as much memory as possible to be dedicated to the fine-tuning script but here it seems like the mlx library is handling it pretty well you know it seems to just dynamically adapt to however much memory is available okay so it computed the V loss I don't think anything's going to blow up so that's great and kudos to Apple and their ml research team for writing a good software library but but I do think since less memory is being allocated to this compared to just not running anything else on my machine this is going to take a lot longer to run before when I was just allowing the training to run all by itself I didn't have the jupyter notebook running either this was hitting like 14 GB of memory 13 to 14 or something got kind of close to 13 there and it took about like 15 or 20 minutes to fine-tune the model with the hyperparameters shown here batch size of four and has 50 training examples so I'm not going to sit here for 20 minutes to wait for this to run cuz I've already done this and I can show you the finished product once again like those cooking shows where they show you how to prepare the food and put in the oven and then magically they had the lasagna that they made last night and they're going to eat it in front of us okay so here's me eating the lasagna we're going to quit out of this process killed that and then what we can do is to run inference with the fine tuned model so this doesn't have the adapter file so what I'll do is open up a new one of these we'll do this in private once training is done so we'll say 20 minutes goes by you go get a sandwich or something while this is running this adapters. npz file will appear in the repository these are the low weights learned during training once that is here we can continue with the Jupiter notebook and we'll use these adapters to run inference again so this will be our fine-tuned model so we're going to run it in a similar way but instead of printing a command and copy pasting it into the terminal here we can just run it in the notebook because there's really not much to see you can see that it failed cuz the adapters file wasn't there but now that it's here we can run this again so now it loaded the pre-trained model it passed in the prompt and the prompt is including the comment which is just that simple great content thank you and then we have the response from Sha GPT glad you enjoyed it smiley face sha GPT so this is much more aligned with something I would actually say to a short comment like this going back this long and poorly formatted comment is not what we want but just after 50 training examples we see that the model is noticeably responding in a different way and then we can even run this a few times to see what else it comes up with glad you liked it so I guess it'll probably keep generating responses that are similar to that yeah glad you're here happy to help okay yeah these are great these are things that I have said in comments so it's doing a good job but that's a really easy comment to respond to let's try a different comment so here's a harder one that at I think this is more recent I don't know this is not in the training data or anything so let's see how sha GPT mlx handles this one so the comment is I discovered your channel yesterday and I'm hooked great job it would be nice to see a video of fine-tuning shot GPT using hugging face I saw a video you running it on Google collap 7B any chance of doing a video on your laptop Mac or using hugging face spaces so that's exactly what this video is let's see what shot GPT thinks hi thanks for your kind words sure thing I'll do a video about fine-tuning HF version on Sha GPT on my Mac then has this YouTube link hope it helps sha GPT okay so let's see what this takes us to ah the video doesn't exist probably because I'm making the video it'd be really crazy if when I post this video this becomes the URL you know it's responding appropriately you know it's like thanking them it's pretty short it puts sha GPT at the end this sentence here doesn't make a whole lot of sense so I'll do a video about fine-tuning HF versions of Shaw GPD so hugging face versions of sha GPD on my Mac see what else it says so I guess this is a kind of hard question to respond to how would it know what I want to do glad to hear it glad you found the channel useful okay well this is a nice response but it doesn't answer the person's question so let's try another one hey glad you're enjoying the channel okay refuses to respond I guess another thing is we can check out the memory spikes during inference so let's do this one more time take a look at the memory pressure so we see that it kind of goes up a bit I guess it's opening up another python instance to run these subprocesses and it takes about 4 GB of memory yeah GL here so it's refusing to respond to this guy's comment I think I got lucky when I uploaded this because it had a really good response let's see glad you enjoyed it I'm looking forward to doing a fine-tuning video on my laptop I've got a Mac M1 mini that runs the latest version of the HF API so the great thing about this one is that it's got right that I have a M1 Mac Mini does run the latest version of the hugging face API but we didn't use the hugging face API but yeah I guess we did so we imported Transformers so Transformers is working under the hood so that was the example super simple but I will say there was one thing I forgot to mention which is that in this L.P file I went through like a dozen different sets of hyperparameters to try to get this thing working which is just the reality of machine learning machine learning is much more art than science or at least for now but I did want to point out one thing that I had to do so I had to go in here and kind of hack one set of hyper parameters okay here so adjusting the rank of these Lowa adapters is not something that this L.P file exposes as a command line argument so you can't just say oh I want to try rank four or rank eight or rank 16 or whatever from the command line I had to go in to the file and just manually change it I think it was eight originally and I changed it to four and this improved the training performance before I did this it was just kept overfitting and I tried a lot of different sets of hyper parameters but reducing the rank worked a lot better and this aligns with results from the low R paper if you've taken a look at and if you haven't check it out it's a really good read rank four rank eight seem to be that sweet spot for the results I guess I can pull it up real quick so in table six of the lowette paper you can see they were comparing what weights they were applying the adapters to and the different ranks of the adapters in the qar example on Google collab I just applied fine tuning to the query layer using rank eight but in this example I applied it to both the query and the value layers and I used rank four and you can see that at least in these examples the rank four rank eight is kind of like this inflection point in the performance like it kind of flattens out and actually starts to get a little worse as the rank gets too big okay so that brings us to the end of this walkthr the example code is again freely available on the GitHub repository shown here and I'll link it in the description below if you enjoyed this video or you have suggest sus for future content please let me know in the comment section below and as always thank you so much for your time and thanks for watching\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_strings = ['&#39;', '&amp;', 'sha ']\n",
        "special_string_replacements = [\"'\", \"&\", \"Shaw \"]\n",
        "\n",
        "for i in range(len(special_strings)):\n",
        "    df = df.with_columns(df['title'].str.replace(special_strings[i], special_string_replacements[i]).alias('title'))\n",
        "    df = df.with_columns(df['transcript'].str.replace(special_strings[i], special_string_replacements[i]).alias('transcript'))"
      ],
      "metadata": {
        "id": "sXA5a8cQ-dDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['title'][9])\n",
        "print(df['transcript'][9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lyXOQVd-reu",
        "outputId": "3af26a11-6908-4647-e90a-e97bc7053c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local LLM Fine-tuning on Mac (M1 16GB)\n",
            "with the rise of open- source models and efficient fine-tuning methods it's never been easier to build custom ml solutions for example anyone with a single GPU can now fine-tune a large language model on their local machine which is exactly what I did in a previous video of this series however since my machine is an M series Mac which doesn't have an Nvidia GPU I had to use the free GPU on Google collab to run that example this is somewhat disappoint pointing because using collabs free gpus is somewhat restrictive and not as convenient as running something on my local machine that's why in this video I'm going to share an easy way to fine-tune an llm locally on Mac and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make if you've been keeping up with machine learning over the past decade then you're probably familiar with Nvidia and all their different gpus it turns out that gpus are super helpful for machine learning because they can much more efficiently train and run machine learning models than traditional CPUs they've also demonstrated an ability to take Nvidia from a hundred billion valuation all the way up to 3 trillion in the past 5 years nvidia's dominance of the GPU Market has greatly influenced the available open-source tools for running and training neural networks the result of this is that a lot of open- source tools work seamlessly with Nvidia Hardware while this is great for Windows and Linux users it often leaves Mac users like me left sitting on these Sidelines after a failed attempt to locally fine-tune llama 2 on my local machine my impression was pulling this off is something that would take several hours of effort that was until I discovered the mlx python library mlx is a python Library developed by Apple's machine learning research team for efficiently running Matrix operations on Apple silicon so we see the documentation here it's inspired by Frameworks like pie torch Jacks and array file one of the notable differences with mlx is that it allows you to use the unified memory model of these M1 chips no longer do you have to worry about RAM and vram being separate things M1 chips just have a single memory so that means me with my Mac Mini M1 2020 with only 16 gigs of memory am capable of fine-tuning a large language model locally on my machine while mlx is a pretty low-level framework it's not going to have highlevel abstractions for loading and training models like hugging face for example there is this example implementation of Laura which is very readily hackable and adaptable to another use case which is exactly what I'm going to do here similar to the previous Cur video here I'm going to fine-tune a quantized version of mistol 7B instruct to respond to YouTube comments in my likeness however instead of using hugging face in Google collab here I'm going to use the mlx library and my local machine and again here are the specs of my machine it's a Mac Mini M1 from 2020 with only 16 GB of memory so by today's standards my machine is hilarious but despite that it's still good enough to implement ment this fine-tuning example so I've put together some example code and that's available on GitHub if you go to my YouTube blog repo go to the llm tab here we see all the different code and videos and blogs of this series so now there's a new one called kulur mlx so we click on that what I've done here is I have this notebook that walks through the example code and then I've taken all the scripts from that mlx example implementation so I've put them in the scripts folder and then here I've prepared data so this is data of my YouTube comments and I prepared it in this Json L format but we won't talk about this right now talk about it a little later okay so the first step in running this example is to go to the repo and clone it so I'll copy this Ur here I'll go over to my terminal let me zoom in a bit okay so we're going to clone the repo get clone might take a while unfortunately there's not a way to clone a specific subfolder of a GitHub repo if you you want to download some code from a repo you have to clone the entire thing go to the repo we just cloned and then the code is in the llms subdirectory and then it's Q mlx so here we see everything we've got our example code requirements file some scripts some data and a readme okay so I cloned the repo and I navigated to this folder so now let's create a python environment that allows us to run the code so here I'll create a new virtual environment called mlx DV and then I'll activate this environment so this how you do it in bash and zsh and I guess every Mac User uses this so this is how you'll activate the environment all right so now you can see we're in this mlx DV environment but the only thing in here is PIP so you see you do pip list all there is is PIP so let's install all the required libraries so we can just do the PIP install D requirements. text and this will install everything while that's happen happening we can look at what is in that so here we have mlx we have mlx LM so this is a library built on top of mlx specifically for large language models also we have the Transformers library and numpy and then finally since I wrote the code in a Jupiter notebook we'll install Jupiter lab in IPI widgets okay so we installed all the requirements and if you run into trouble in the installation steps so here are some important notes from mlx is documentation first and foremost you need the M series chip like that's the whole point of this video and this Library the second is that you are using a native python that's greater than or equal to 3.8 so I think here I'm using 3.12 if I just do this yeah so I'm using 3.2.2 then you have to have at least Mac OS 13.5 but they recommend that you have Mac OS 14 which is what I'm running on my machine right now so now that we've cloned the repo and we set up our environment let's run through the example code so to do that we'll do Jupiter lab okay so here we've got the example code video link and blog link coming soon cuz I'm making it right now and then we're going to zoom in because it's probably tiny on your screen all right so mlx fine tuning we're going to do some imports so we're going to import subprocess because all the example code that we're hacking runs from the command line and so subprocess allows us to run terminal commands through Python and then here I'm importing the mlx LM library to run inference on a model a little later so I defined some helper functions here they're not super important right now so I'll just come back to them as we encounter them in the code and then this is an optional step that example code from mlx comes with this convert. py script which is capable of taking any model from the hugging face Hub and converting it into the proper mlx format and additionally quantizing it so I actually did this for mistol 7B instruct version 0.2 and then there there's this argument that you can pass that will push the model to the mlx community so I guess that's worth calling out right now mlx has this page on hugging face here are several mlx compatible models many of which are quantized so there's Google Gemma mistol quen code Gemma llama 3 53 whisper llama 3.1 even so this page seems pretty active and basically any model that you would want to find tune is probably already available here and if it's not you can simply just go find the model that you want to use in mlx let's go to mistal AI I'll go to Mistral 7B instruct so this is the one I used given this hugging face model path we can convert it to the mlx format and quantize it using this convert. py script so there was this typo so I had to add this quantize flag so what this will do is grab the hugging face model convert to the mlx format and this quantize flag converts into 4bit quanti ization so I actually didn't run the command yet so you could run this command using the subprocess module but I found it's better to just print the command like this and copy paste it into the terminal because when you run these shell commands in a jupyter notebook you don't see the same like progress metrics that you would normally and this will actually save a mlx version of that model on your local machine that you can run for inference and fine-tuning since here we're going to use a model that's already available on the hugging face hub here's the model card for it we can just skip this quantized model step and so here what's happening is I'm going to build the prompt that is defined here so again here we're creating a YouTube comment responder what this model will do is you'll pass in a comment and it'll respond to that comment hopefully in my likeness so for this example I'm not just going to pass the raw comment into the model itself for inference I have actually constructed this prompt which is the same prompt that we saw in the previous Cur example to help the model generate better responses I created a Lambda function for this to find this instruction string and then did some string manipulation to incorporate the comment into it dynamically the result of that is this prompt Builder Lambda function which takes in the comment and so here we're just doing a very boring comment great content thank you and then we'll Define the max number of tokens and then here we're going to use the mlx LM library to do inference so the syntax looks very similar to hugging face which is pretty convenient if you're comfortable with hugging face already so here what we're going to do is we're going to load in the model so this is that same quantize model and then we're going to have the model generate a response so we pass in the model the tokenizer which was loaded automatically The Prompt that we defined using our prompt Builder the max number of tokens which I set to 140 and then I put verbos equal to True okay so we can take a look at the prompt here which is all this stuff I'm not going to read all this you can read it if you like but this is just to help nudge the model in the right direction then we have this please respond to the following comment and this is the comment that we spliced in using that prompt Builder Lambda function and then down here is the model's response so this is the raw quantized model no fine-tuning whatsoever and this is the response well first and foremost it put sha GPT in the wrong place this is supposed to be at the end of the response not at the beginning but it says thank you for your kind words I'm glad you found the content helpful and enjoyable if you any specific questions or topics you'd like me to cover in more detail please feel free to ask I would never respond to a comment like this this is something we've seen in the previous fine-tuning videos where the unfin tuned responses tend to be pretty verbose and when I respond to comments or really any kind of communication I try to keep it as concise as possible so this is very different than something I'd actually write in response to a comment so let's see how we can fine-tune this model to generate responses that sound a lot more like me here again we're going to use one of the scripts from that mlx examples repo and I'm going to construct the command in the same way so I put everything in a list and the reason I have it in a list is because that's how you can run these terminal commands in Python you'll pass it in as a list to the subprocess module but I realized that even with this fancy helper function that chat GPT wrote me to continuously print outputs from my terminal command it still wasn't printing the loss during training so I actually found an alternative strategy to be more helpful which is to just take this command variable here and to translate it into a string that I can copy and paste into my terminal and so this construct shell command is just a pretty simple helper function up here that's just doing some basic string manipulation to convert this list into a string what I'll do is I'll copy this string and then we can go over to terminal and then I'll paste the command here here okay so walking through this a little bit I'll try to zoom in even more hopefully that's legible we're running a python script so that's what this is the python script we're running is called lowra and it's in this scripts subdirectory and then we're just going to define the parameters for training so here we're specifying the model which is that 4-bit version of mistal 7B instruct we have this train flag because we're going to run training we're going to set the number of iterations so we're going to do 100 iterations that means it's going to run through 100 batches and the default batch is four next is steps per eval so this is the number of batches that the training will run through before Computing the validation loss so I set it to 10 which is the same as the number of steps per training loss evaluation the Val batches is the number of examples to include in the validation loss calculation setting it to ne1 goes through everything in the validation data set and here that's only 10 examples next we set the learning rate at 1 to the minus 5 which is actually the same as the default but I have it explicitly written here cuz I was playing around with this I probably ran this a dozen different times trying to find the best hyper parameters and then low R layers 16 which is also the default parameter but I went through a lot of iterations to just end up coming back to the default and then finally we use this test flag which computes the test loss at the end of training I guess before we run this it's worth talking about the data that we're using here we have three data sets so we have have this train. Json L test. Json L and valid Json L the way I make these data sets are here in this jupyter notebook also available on the GitHub what I'm doing here is I'm taking a CSV file of YouTube comments and responses which looks like this so I've got 70 comments these are real comments and 70 real responses from me and so this is the way I'm going to train the model to respond to comments in my likeness and way I do the train test validation split is I have 50 examples for training 10 examples for testing and 10 examples for validation I won't walk through this code cuz I did it in the Cur video and I did a similar thing for the open AI fine-tuning API video but if you're curious about how I'm doing the data preparation feel free to check this out we're using the same prompt in the training data as I used at inference in the example code we just saw very similar strategy the Json l format if you're familiar with python is essentially a list of dictionaries so a dictionary is a set of key value Pairs and a list is just a sequence a collection of elements Json L is just going to be a collection of these key value pairs where each key value pair each dictionary consists of one key and one value so super simple the key is text and the value is the example that you want to use to train or evaluate the language model notice that this contains the instructions essentially of the model this bit is the comment the real comment from the user and then it ends with this instruction token and then here's the real response from me so all of these are packed together to form the example and we have 70 of these and 50 are going toward training 10 are going for testing and 10 are going for validation so here I randomly select examples for testing and validation and then I just write everything to these Json L files so hopefully this example code is easy for you to follow and hack and adapt for your own use case and if you get stuck you know feel free to drop a comment or reach out I'm happy to help try to get you unstuck okay so that brief aside was the data preparation now let's go into training so I actually haven't done this while running OBS so my computer might blow up if I do this so I've got activity monitor and it's not looking great because OBS is already using a gig of memory and when I was running this the fine-tuning script was taking like 14 G gabt of memory and so I'm going to execute this script but if my computer blows up and I'm not able to post this video I'm so sorry all right here goes nothing nothing has blown up yet we'll just keep the activity monitor here so we can monitor the memory pressure so we see that the fine-tuning script is taking about 10 gbes of memory there's this other python script which I guess is the Jupiter notebook taking up 4 GB of memory M and then we got OBS here taking up 1 gab of memory so when I was doing this for real I was basically doing nothing else on my machine I was just allowing as much memory as possible to be dedicated to the fine-tuning script but here it seems like the mlx library is handling it pretty well you know it seems to just dynamically adapt to however much memory is available okay so it computed the V loss I don't think anything's going to blow up so that's great and kudos to Apple and their ml research team for writing a good software library but but I do think since less memory is being allocated to this compared to just not running anything else on my machine this is going to take a lot longer to run before when I was just allowing the training to run all by itself I didn't have the jupyter notebook running either this was hitting like 14 GB of memory 13 to 14 or something got kind of close to 13 there and it took about like 15 or 20 minutes to fine-tune the model with the hyperparameters shown here batch size of four and has 50 training examples so I'm not going to sit here for 20 minutes to wait for this to run cuz I've already done this and I can show you the finished product once again like those cooking shows where they show you how to prepare the food and put in the oven and then magically they had the lasagna that they made last night and they're going to eat it in front of us okay so here's me eating the lasagna we're going to quit out of this process killed that and then what we can do is to run inference with the fine tuned model so this doesn't have the adapter file so what I'll do is open up a new one of these we'll do this in private once training is done so we'll say 20 minutes goes by you go get a sandwich or something while this is running this adapters. npz file will appear in the repository these are the low weights learned during training once that is here we can continue with the Jupiter notebook and we'll use these adapters to run inference again so this will be our fine-tuned model so we're going to run it in a similar way but instead of printing a command and copy pasting it into the terminal here we can just run it in the notebook because there's really not much to see you can see that it failed cuz the adapters file wasn't there but now that it's here we can run this again so now it loaded the pre-trained model it passed in the prompt and the prompt is including the comment which is just that simple great content thank you and then we have the response from Sha GPT glad you enjoyed it smiley face sha GPT so this is much more aligned with something I would actually say to a short comment like this going back this long and poorly formatted comment is not what we want but just after 50 training examples we see that the model is noticeably responding in a different way and then we can even run this a few times to see what else it comes up with glad you liked it so I guess it'll probably keep generating responses that are similar to that yeah glad you're here happy to help okay yeah these are great these are things that I have said in comments so it's doing a good job but that's a really easy comment to respond to let's try a different comment so here's a harder one that at I think this is more recent I don't know this is not in the training data or anything so let's see how sha GPT mlx handles this one so the comment is I discovered your channel yesterday and I'm hooked great job it would be nice to see a video of fine-tuning shot GPT using hugging face I saw a video you running it on Google collap 7B any chance of doing a video on your laptop Mac or using hugging face spaces so that's exactly what this video is let's see what shot GPT thinks hi thanks for your kind words sure thing I'll do a video about fine-tuning HF version on Sha GPT on my Mac then has this YouTube link hope it helps sha GPT okay so let's see what this takes us to ah the video doesn't exist probably because I'm making the video it'd be really crazy if when I post this video this becomes the URL you know it's responding appropriately you know it's like thanking them it's pretty short it puts sha GPT at the end this sentence here doesn't make a whole lot of sense so I'll do a video about fine-tuning HF versions of Shaw GPD so hugging face versions of sha GPD on my Mac see what else it says so I guess this is a kind of hard question to respond to how would it know what I want to do glad to hear it glad you found the channel useful okay well this is a nice response but it doesn't answer the person's question so let's try another one hey glad you're enjoying the channel okay refuses to respond I guess another thing is we can check out the memory spikes during inference so let's do this one more time take a look at the memory pressure so we see that it kind of goes up a bit I guess it's opening up another python instance to run these subprocesses and it takes about 4 GB of memory yeah GL here so it's refusing to respond to this guy's comment I think I got lucky when I uploaded this because it had a really good response let's see glad you enjoyed it I'm looking forward to doing a fine-tuning video on my laptop I've got a Mac M1 mini that runs the latest version of the HF API so the great thing about this one is that it's got right that I have a M1 Mac Mini does run the latest version of the hugging face API but we didn't use the hugging face API but yeah I guess we did so we imported Transformers so Transformers is working under the hood so that was the example super simple but I will say there was one thing I forgot to mention which is that in this L.P file I went through like a dozen different sets of hyperparameters to try to get this thing working which is just the reality of machine learning machine learning is much more art than science or at least for now but I did want to point out one thing that I had to do so I had to go in here and kind of hack one set of hyper parameters okay here so adjusting the rank of these Lowa adapters is not something that this L.P file exposes as a command line argument so you can't just say oh I want to try rank four or rank eight or rank 16 or whatever from the command line I had to go in to the file and just manually change it I think it was eight originally and I changed it to four and this improved the training performance before I did this it was just kept overfitting and I tried a lot of different sets of hyper parameters but reducing the rank worked a lot better and this aligns with results from the low R paper if you've taken a look at and if you haven't check it out it's a really good read rank four rank eight seem to be that sweet spot for the results I guess I can pull it up real quick so in table six of the lowette paper you can see they were comparing what weights they were applying the adapters to and the different ranks of the adapters in the qar example on Google collab I just applied fine tuning to the query layer using rank eight but in this example I applied it to both the query and the value layers and I used rank four and you can see that at least in these examples the rank four rank eight is kind of like this inflection point in the performance like it kind of flattens out and actually starts to get a little worse as the rank gets too big okay so that brings us to the end of this walkthr the example code is again freely available on the GitHub repository shown here and I'll link it in the description below if you enjoyed this video or you have suggest sus for future content please let me know in the comment section below and as always thank you so much for your time and thanks for watching\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write data to file\n",
        "df.write_parquet('/content/video-transcripts.parquet')\n",
        "df.write_csv('/content/video-transcripts.csv')"
      ],
      "metadata": {
        "id": "VvI-1v1U-2cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare Candidate Search Approaches**"
      ],
      "metadata": {
        "id": "U3lzHRxiAjwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "from sklearn.metrics import DistanceMetric\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4PR-iEDDAfu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pl.read_parquet('/content/video-transcripts.parquet')\n",
        "df_eval = pl.read_csv('/content/eval_data.csv')\n",
        "df_eval.head()"
      ],
      "metadata": {
        "id": "M0fI7JbkAwrd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "63755f15-119e-430e-a82a-dcd967a80964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 2)\n",
              "┌───────────────────────────────────┬─────────────┐\n",
              "│ query                             ┆ video_id    │\n",
              "│ ---                               ┆ ---         │\n",
              "│ str                               ┆ str         │\n",
              "╞═══════════════════════════════════╪═════════════╡\n",
              "│  left my sixf figure data scienc… ┆ 7Oy2NmPwJXo │\n",
              "│  explained in 60 seconds knowled… ┆ ZVVkdXHqEuM │\n",
              "│ AI                                ┆ 9joIFeKuf04 │\n",
              "│ Fine consulting                   ┆ pNg2DJ4spXg │\n",
              "│ LLM from scratch                  ┆ FLkUOkeMd5M │\n",
              "└───────────────────────────────────┴─────────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>query</th><th>video_id</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot; left my sixf …</td><td>&quot;7Oy2NmPwJXo&quot;</td></tr><tr><td>&quot; explained in …</td><td>&quot;ZVVkdXHqEuM&quot;</td></tr><tr><td>&quot;AI&quot;</td><td>&quot;9joIFeKuf04&quot;</td></tr><tr><td>&quot;Fine consultin…</td><td>&quot;pNg2DJ4spXg&quot;</td></tr><tr><td>&quot;LLM from scrat…</td><td>&quot;FLkUOkeMd5M&quot;</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpybzu0UtUZQ",
        "outputId": "fdb96542-8132-4af6-efd3-7b4b060948ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**embed titles and transcripts**"
      ],
      "metadata": {
        "id": "TkrQbnF6ikOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define \"parameters\"\n",
        "column_to_embed_list = ['title', 'transcript']\n",
        "model_name_list = [\"all-MiniLM-L6-v2\", \"multi-qa-distilbert-cos-v1\", \"multi-qa-mpnet-base-dot-v1\"]"
      ],
      "metadata": {
        "id": "yBPvLKF6i790"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate embeddings for each combination of column and model\n",
        "\n",
        "# initialize dict to keep track of all text embeddings\n",
        "text_embedding_dict = {}\n",
        "\n",
        "for model_name in model_name_list:\n",
        "\n",
        "    #define embedding model\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    for column_name in column_to_embed_list:\n",
        "\n",
        "        # define text embedding identifier\n",
        "        key_name = model_name + \"_\" + column_name\n",
        "        print(key_name)\n",
        "\n",
        "        # generate embeddings for text under column_name\n",
        "        %time embedding_arr = model.encode(df[column_name].to_list())\n",
        "        print('')\n",
        "\n",
        "        # append embeddings to dict\n",
        "        text_embedding_dict[key_name] = embedding_arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8nk2uoHkG9w",
        "outputId": "f77caea7-3ef6-4939-a146-2ecc3316f0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all-MiniLM-L6-v2_title\n",
            "CPU times: user 1.08 s, sys: 26.5 ms, total: 1.11 s\n",
            "Wall time: 1.07 s\n",
            "\n",
            "all-MiniLM-L6-v2_transcript\n",
            "CPU times: user 15.7 s, sys: 1.96 s, total: 17.7 s\n",
            "Wall time: 17.1 s\n",
            "\n",
            "multi-qa-distilbert-cos-v1_title\n",
            "CPU times: user 4.94 s, sys: 40.7 ms, total: 4.98 s\n",
            "Wall time: 5.07 s\n",
            "\n",
            "multi-qa-distilbert-cos-v1_transcript\n",
            "CPU times: user 1min 44s, sys: 44.3 s, total: 2min 28s\n",
            "Wall time: 2min 29s\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.6.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi-qa-mpnet-base-dot-v1_title\n",
            "CPU times: user 8.48 s, sys: 52.4 ms, total: 8.53 s\n",
            "Wall time: 8.63 s\n",
            "\n",
            "multi-qa-mpnet-base-dot-v1_transcript\n",
            "CPU times: user 3min 26s, sys: 1min 21s, total: 4min 47s\n",
            "Wall time: 4min 49s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**embed queries**"
      ],
      "metadata": {
        "id": "VjpzqdjIlFf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_embedding_dict = {}\n",
        "\n",
        "for model_name in model_name_list:\n",
        "\n",
        "    #define embedding model\n",
        "    model = SentenceTransformer(model_name)\n",
        "    print(model_name)\n",
        "\n",
        "    # embed query text\n",
        "    %time embedding_arr = model.encode(df_eval['query'].to_list())\n",
        "    print('')\n",
        "\n",
        "    # append embedding to dict\n",
        "    query_embedding_dict[model_name] = embedding_arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJid2rCHlHD6",
        "outputId": "f2468b22-1072-4b1b-d377-d0d2f56904f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all-MiniLM-L6-v2\n",
            "CPU times: user 862 ms, sys: 26.5 ms, total: 888 ms\n",
            "Wall time: 890 ms\n",
            "\n",
            "multi-qa-distilbert-cos-v1\n",
            "CPU times: user 3.05 s, sys: 31.6 ms, total: 3.08 s\n",
            "Wall time: 3.13 s\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.6.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multi-qa-mpnet-base-dot-v1\n",
            "CPU times: user 4.64 s, sys: 53 ms, total: 4.69 s\n",
            "Wall time: 4.63 s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate search methods**"
      ],
      "metadata": {
        "id": "WJWvuJYWlPXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def returnVideoID_index(df: pl.dataframe.frame.DataFrame, df_eval: pl.dataframe.frame.DataFrame, query_n: int) -> int:\n",
        "    \"\"\"\n",
        "        Function to return the index of a dataframe corresponding to the nth row in evaluation dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    return [i for i in range(len(df)) if df['video_id'][i]==df_eval['video_id'][query_n]][0]"
      ],
      "metadata": {
        "id": "p5BjYm1flVTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalTrueRankings(dist_arr_isorted: np.ndarray, df: pl.dataframe.frame.DataFrame, df_eval: pl.dataframe.frame.DataFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "        Function to return \"true\" video ID rankings for each evaluation query\n",
        "    \"\"\"\n",
        "\n",
        "    # intialize array to store rankings of \"correct\" search result\n",
        "    true_rank_arr = np.empty((1, dist_arr_isorted.shape[1]))\n",
        "\n",
        "    # evaluate ranking of correct result for each query\n",
        "    for query_n in range(dist_arr_isorted.shape[1]):\n",
        "\n",
        "        # return \"true\" video ID's in df\n",
        "        video_id_idx = returnVideoID_index(df, df_eval, query_n)\n",
        "\n",
        "        # evaluate the ranking of the \"true\" video ID\n",
        "        true_rank = np.argwhere(dist_arr_isorted[:,query_n]==video_id_idx)[0][0]\n",
        "\n",
        "        # store the \"true\" video ID's ranking in array\n",
        "        true_rank_arr[0,query_n] = true_rank\n",
        "\n",
        "    return true_rank_arr"
      ],
      "metadata": {
        "id": "OgGM8KK4lZTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize distance metrics to experiment\n",
        "dist_name_list = ['euclidean', 'manhattan', 'chebyshev']\n",
        "sim_name_list = ['cos_sim', 'dot_score']"
      ],
      "metadata": {
        "id": "NxQ1DV8ZmURC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df_eval))\n",
        "print(query_n)  # Ensure this is less than len(df_eval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "dAvtO81K7l4b",
        "outputId": "ea82a0f2-41bf-49b6-a42e-bd2b4338f3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'query_n' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_1171/1826158702.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_n\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure this is less than len(df_eval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'query_n' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate all possible combinations of model, columns to embed, and distance metrics\n",
        "\n",
        "# initialize list to store results\n",
        "eval_results = []\n",
        "\n",
        "# loop through all models\n",
        "for model_name in model_name_list:\n",
        "\n",
        "    # generate query embedding\n",
        "    query_embedding = query_embedding_dict[model_name]\n",
        "\n",
        "    # loop through text columns\n",
        "    for column_name in column_to_embed_list:\n",
        "\n",
        "        # generate column embedding\n",
        "        embedding_arr = text_embedding_dict[model_name+'_'+column_name]\n",
        "\n",
        "        # loop through distance metrics\n",
        "        for dist_name in dist_name_list:\n",
        "\n",
        "            # compute distance between video text and query\n",
        "            dist = DistanceMetric.get_metric(dist_name)\n",
        "            dist_arr = dist.pairwise(embedding_arr, query_embedding)\n",
        "\n",
        "            # sort indexes of distance array\n",
        "            dist_arr_isorted = np.argsort(dist_arr, axis=0)\n",
        "\n",
        "            # define label for search method\n",
        "            method_name = \"_\".join([model_name, column_name, dist_name])\n",
        "\n",
        "            # evaluate the ranking of the ground truth\n",
        "            true_rank_arr = evalTrueRankings(dist_arr_isorted, df, df_eval)\n",
        "\n",
        "            # store results\n",
        "            eval_list = [method_name] + true_rank_arr.tolist()[0]\n",
        "            eval_results.append(eval_list)\n",
        "\n",
        "        # loop through sbert similarity scores\n",
        "        for sim_name in sim_name_list:\n",
        "            # apply similarity score from sbert\n",
        "            cmd = \"dist_arr = -util.\" + sim_name + \"(embedding_arr, query_embedding)\"\n",
        "            exec(cmd)\n",
        "\n",
        "            # sort indexes of distance array (notice minus sign in front of cosine similarity)\n",
        "            dist_arr_isorted = np.argsort(dist_arr, axis=0)\n",
        "\n",
        "            # define label for search method\n",
        "            method_name = \"_\".join([model_name, column_name, sim_name.replace(\"_\",\"-\")])\n",
        "\n",
        "            # evaluate the ranking of the ground truth\n",
        "            true_rank_arr = evalTrueRankings(dist_arr_isorted, df, df_eval)\n",
        "\n",
        "            # store results\n",
        "            eval_list = [method_name] + true_rank_arr.tolist()[0]\n",
        "            eval_results.append(eval_list)\n"
      ],
      "metadata": {
        "id": "uN6kp2VPokr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VpF068ErrWDK",
        "outputId": "fc38f200-5297-4f85-cffc-37c5936510fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dist_arr = -util.dot_score(embedding_arr, query_embedding)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute rankings for title + transcripts embedding\n",
        "for model_name in model_name_list:\n",
        "\n",
        "    # generate embeddings\n",
        "    embedding_arr1 = text_embedding_dict[model_name+'_title']\n",
        "    embedding_arr2 = text_embedding_dict[model_name+'_transcript']\n",
        "    query_embedding = query_embedding_dict[model_name]\n",
        "\n",
        "    for dist_name in dist_name_list:\n",
        "\n",
        "        # compute distance between video text and query\n",
        "        dist = DistanceMetric.get_metric(dist_name)\n",
        "        dist_arr = dist.pairwise(embedding_arr1, query_embedding) + dist.pairwise(embedding_arr2, query_embedding)\n",
        "\n",
        "        # sort indexes of distance array\n",
        "        dist_arr_isorted = np.argsort(dist_arr, axis=0)\n",
        "\n",
        "         # define label for search method\n",
        "        method_name = \"_\".join([model_name, \"title-transcript\", dist_name])\n",
        "\n",
        "        # evaluate the ranking of the ground truth\n",
        "        true_rank_arr = evalTrueRankings(dist_arr_isorted, df, df_eval)\n",
        "\n",
        "        # store results\n",
        "        eval_list = [method_name] + true_rank_arr.tolist()[0]\n",
        "        eval_results.append(eval_list)\n",
        "\n",
        "    # loop through sbert similarity scores\n",
        "    for sim_name in sim_name_list:\n",
        "        # apply similarity score from sbert\n",
        "        cmd = \"dist_arr = -util.\" + sim_name + \"(embedding_arr1, query_embedding) - util.\"+ sim_name + \"(embedding_arr2, query_embedding)\"\n",
        "        exec(cmd)\n",
        "\n",
        "        # sort indexes of distance array (notice minus sign in front of cosine similarity)\n",
        "        dist_arr_isorted = np.argsort(dist_arr, axis=0)\n",
        "\n",
        "        # define label for search method\n",
        "        method_name = \"_\".join([model_name, \"title-transcript\", sim_name.replace(\"_\",\"-\")])\n",
        "\n",
        "        # evaluate the ranking of the ground truth\n",
        "        true_rank_arr = evalTrueRankings(dist_arr_isorted, df, df_eval)\n",
        "\n",
        "        # store results\n",
        "        eval_list = [method_name] + true_rank_arr.tolist()[0]\n",
        "        eval_results.append(eval_list)"
      ],
      "metadata": {
        "id": "PrAp0g9Wrblr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuHJcqKHrh-6",
        "outputId": "172e2100-ffb1-4ba6-83c1-47566d0a7223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define schema for results dataframe\n",
        "schema_dict = {'method_name':str}\n",
        "for i in range(len(eval_results[0])-1):\n",
        "    schema_dict['rank_query-'+str(i)] = float\n",
        "\n",
        "# store results in dataframe\n",
        "df_results = pl.DataFrame(eval_results, schema=schema_dict)\n",
        "df_results.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "wk9EU4buropC",
        "outputId": "681ea2a7-7fb7-4823-ed4b-46f95fac72c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 70)\n",
              "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
              "│ method_na ┆ rank_quer ┆ rank_quer ┆ rank_quer ┆ … ┆ rank_quer ┆ rank_quer ┆ rank_quer ┆ rank_que │\n",
              "│ me        ┆ y-0       ┆ y-1       ┆ y-2       ┆   ┆ y-65      ┆ y-66      ┆ y-67      ┆ ry-68    │\n",
              "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
              "│ str       ┆ f64       ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
              "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
              "│ all-MiniL ┆ 15.0      ┆ 2.0       ┆ 22.0      ┆ … ┆ 18.0      ┆ 2.0       ┆ 68.0      ┆ 48.0     │\n",
              "│ M-L6-v2_t ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ itle_eucl ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ idean     ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ all-MiniL ┆ 16.0      ┆ 2.0       ┆ 27.0      ┆ … ┆ 20.0      ┆ 2.0       ┆ 71.0      ┆ 45.0     │\n",
              "│ M-L6-v2_t ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ itle_manh ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ attan     ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ all-MiniL ┆ 14.0      ┆ 2.0       ┆ 21.0      ┆ … ┆ 20.0      ┆ 2.0       ┆ 19.0      ┆ 44.0     │\n",
              "│ M-L6-v2_t ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ itle_cheb ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ yshev     ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ all-MiniL ┆ 15.0      ┆ 2.0       ┆ 22.0      ┆ … ┆ 18.0      ┆ 2.0       ┆ 68.0      ┆ 48.0     │\n",
              "│ M-L6-v2_t ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ itle_cos- ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ sim       ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ all-MiniL ┆ 15.0      ┆ 2.0       ┆ 22.0      ┆ … ┆ 18.0      ┆ 2.0       ┆ 68.0      ┆ 48.0     │\n",
              "│ M-L6-v2_t ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ itle_dot- ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ score     ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 70)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>method_name</th><th>rank_query-0</th><th>rank_query-1</th><th>rank_query-2</th><th>rank_query-3</th><th>rank_query-4</th><th>rank_query-5</th><th>rank_query-6</th><th>rank_query-7</th><th>rank_query-8</th><th>rank_query-9</th><th>rank_query-10</th><th>rank_query-11</th><th>rank_query-12</th><th>rank_query-13</th><th>rank_query-14</th><th>rank_query-15</th><th>rank_query-16</th><th>rank_query-17</th><th>rank_query-18</th><th>rank_query-19</th><th>rank_query-20</th><th>rank_query-21</th><th>rank_query-22</th><th>rank_query-23</th><th>rank_query-24</th><th>rank_query-25</th><th>rank_query-26</th><th>rank_query-27</th><th>rank_query-28</th><th>rank_query-29</th><th>rank_query-30</th><th>rank_query-31</th><th>rank_query-32</th><th>rank_query-33</th><th>rank_query-34</th><th>rank_query-35</th><th>rank_query-36</th><th>rank_query-37</th><th>rank_query-38</th><th>rank_query-39</th><th>rank_query-40</th><th>rank_query-41</th><th>rank_query-42</th><th>rank_query-43</th><th>rank_query-44</th><th>rank_query-45</th><th>rank_query-46</th><th>rank_query-47</th><th>rank_query-48</th><th>rank_query-49</th><th>rank_query-50</th><th>rank_query-51</th><th>rank_query-52</th><th>rank_query-53</th><th>rank_query-54</th><th>rank_query-55</th><th>rank_query-56</th><th>rank_query-57</th><th>rank_query-58</th><th>rank_query-59</th><th>rank_query-60</th><th>rank_query-61</th><th>rank_query-62</th><th>rank_query-63</th><th>rank_query-64</th><th>rank_query-65</th><th>rank_query-66</th><th>rank_query-67</th><th>rank_query-68</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;all-MiniLM-L6-…</td><td>15.0</td><td>2.0</td><td>22.0</td><td>38.0</td><td>7.0</td><td>89.0</td><td>78.0</td><td>43.0</td><td>97.0</td><td>35.0</td><td>59.0</td><td>17.0</td><td>102.0</td><td>63.0</td><td>7.0</td><td>21.0</td><td>43.0</td><td>39.0</td><td>99.0</td><td>6.0</td><td>59.0</td><td>47.0</td><td>26.0</td><td>93.0</td><td>36.0</td><td>15.0</td><td>2.0</td><td>83.0</td><td>76.0</td><td>63.0</td><td>21.0</td><td>23.0</td><td>67.0</td><td>3.0</td><td>39.0</td><td>41.0</td><td>69.0</td><td>1.0</td><td>44.0</td><td>17.0</td><td>75.0</td><td>5.0</td><td>100.0</td><td>4.0</td><td>3.0</td><td>73.0</td><td>87.0</td><td>10.0</td><td>24.0</td><td>4.0</td><td>84.0</td><td>100.0</td><td>23.0</td><td>8.0</td><td>52.0</td><td>62.0</td><td>87.0</td><td>98.0</td><td>1.0</td><td>18.0</td><td>36.0</td><td>104.0</td><td>59.0</td><td>33.0</td><td>7.0</td><td>18.0</td><td>2.0</td><td>68.0</td><td>48.0</td></tr><tr><td>&quot;all-MiniLM-L6-…</td><td>16.0</td><td>2.0</td><td>27.0</td><td>41.0</td><td>7.0</td><td>91.0</td><td>84.0</td><td>45.0</td><td>99.0</td><td>26.0</td><td>40.0</td><td>19.0</td><td>90.0</td><td>59.0</td><td>6.0</td><td>26.0</td><td>53.0</td><td>35.0</td><td>100.0</td><td>6.0</td><td>70.0</td><td>58.0</td><td>29.0</td><td>92.0</td><td>39.0</td><td>15.0</td><td>2.0</td><td>98.0</td><td>72.0</td><td>66.0</td><td>20.0</td><td>17.0</td><td>65.0</td><td>3.0</td><td>46.0</td><td>39.0</td><td>84.0</td><td>1.0</td><td>58.0</td><td>16.0</td><td>79.0</td><td>5.0</td><td>102.0</td><td>4.0</td><td>3.0</td><td>97.0</td><td>80.0</td><td>12.0</td><td>19.0</td><td>7.0</td><td>91.0</td><td>102.0</td><td>26.0</td><td>5.0</td><td>53.0</td><td>53.0</td><td>79.0</td><td>90.0</td><td>2.0</td><td>28.0</td><td>46.0</td><td>101.0</td><td>54.0</td><td>40.0</td><td>7.0</td><td>20.0</td><td>2.0</td><td>71.0</td><td>45.0</td></tr><tr><td>&quot;all-MiniLM-L6-…</td><td>14.0</td><td>2.0</td><td>21.0</td><td>40.0</td><td>14.0</td><td>98.0</td><td>29.0</td><td>60.0</td><td>99.0</td><td>40.0</td><td>70.0</td><td>44.0</td><td>94.0</td><td>97.0</td><td>21.0</td><td>12.0</td><td>69.0</td><td>89.0</td><td>93.0</td><td>13.0</td><td>70.0</td><td>43.0</td><td>19.0</td><td>77.0</td><td>32.0</td><td>101.0</td><td>2.0</td><td>57.0</td><td>104.0</td><td>78.0</td><td>95.0</td><td>34.0</td><td>46.0</td><td>1.0</td><td>18.0</td><td>20.0</td><td>28.0</td><td>1.0</td><td>71.0</td><td>29.0</td><td>35.0</td><td>7.0</td><td>89.0</td><td>3.0</td><td>8.0</td><td>25.0</td><td>73.0</td><td>10.0</td><td>97.0</td><td>1.0</td><td>67.0</td><td>70.0</td><td>52.0</td><td>58.0</td><td>20.0</td><td>71.0</td><td>86.0</td><td>81.0</td><td>2.0</td><td>39.0</td><td>27.0</td><td>96.0</td><td>30.0</td><td>35.0</td><td>10.0</td><td>20.0</td><td>2.0</td><td>19.0</td><td>44.0</td></tr><tr><td>&quot;all-MiniLM-L6-…</td><td>15.0</td><td>2.0</td><td>22.0</td><td>38.0</td><td>7.0</td><td>89.0</td><td>78.0</td><td>43.0</td><td>97.0</td><td>35.0</td><td>59.0</td><td>17.0</td><td>102.0</td><td>63.0</td><td>7.0</td><td>21.0</td><td>43.0</td><td>39.0</td><td>99.0</td><td>6.0</td><td>59.0</td><td>47.0</td><td>26.0</td><td>93.0</td><td>36.0</td><td>15.0</td><td>2.0</td><td>83.0</td><td>76.0</td><td>63.0</td><td>21.0</td><td>23.0</td><td>67.0</td><td>3.0</td><td>39.0</td><td>41.0</td><td>69.0</td><td>1.0</td><td>44.0</td><td>17.0</td><td>75.0</td><td>5.0</td><td>100.0</td><td>4.0</td><td>3.0</td><td>73.0</td><td>87.0</td><td>10.0</td><td>24.0</td><td>4.0</td><td>84.0</td><td>100.0</td><td>23.0</td><td>8.0</td><td>52.0</td><td>62.0</td><td>87.0</td><td>98.0</td><td>1.0</td><td>18.0</td><td>36.0</td><td>104.0</td><td>59.0</td><td>33.0</td><td>7.0</td><td>18.0</td><td>2.0</td><td>68.0</td><td>48.0</td></tr><tr><td>&quot;all-MiniLM-L6-…</td><td>15.0</td><td>2.0</td><td>22.0</td><td>38.0</td><td>7.0</td><td>89.0</td><td>78.0</td><td>43.0</td><td>97.0</td><td>35.0</td><td>59.0</td><td>17.0</td><td>102.0</td><td>63.0</td><td>7.0</td><td>21.0</td><td>43.0</td><td>39.0</td><td>99.0</td><td>6.0</td><td>59.0</td><td>47.0</td><td>26.0</td><td>93.0</td><td>36.0</td><td>15.0</td><td>2.0</td><td>83.0</td><td>76.0</td><td>63.0</td><td>21.0</td><td>23.0</td><td>67.0</td><td>3.0</td><td>39.0</td><td>41.0</td><td>69.0</td><td>1.0</td><td>44.0</td><td>17.0</td><td>75.0</td><td>5.0</td><td>100.0</td><td>4.0</td><td>3.0</td><td>73.0</td><td>87.0</td><td>10.0</td><td>24.0</td><td>4.0</td><td>84.0</td><td>100.0</td><td>23.0</td><td>8.0</td><td>52.0</td><td>62.0</td><td>87.0</td><td>98.0</td><td>1.0</td><td>18.0</td><td>36.0</td><td>104.0</td><td>59.0</td><td>33.0</td><td>7.0</td><td>18.0</td><td>2.0</td><td>68.0</td><td>48.0</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute mean rankings of ground truth search result\n",
        "df_results = df_results.with_columns(new_col=pl.mean_horizontal(df_results.columns[1:])).rename({\"new_col\": \"rank_query-mean\"})"
      ],
      "metadata": {
        "id": "FiMm7U2Pry-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute number of ground truth results which appear in top 3\n",
        "for i in [1,3]:\n",
        "    df_results = df_results.with_columns(new_col=pl.sum_horizontal(df_results[:,1:-1]<i)).rename({\"new_col\": \"num_in_top-\"+str(i)})"
      ],
      "metadata": {
        "id": "VPuyDT_Er1ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Look at top results**"
      ],
      "metadata": {
        "id": "72FDWTmJr6wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary = df_results[['method_name', \"rank_query-mean\", \"num_in_top-1\", \"num_in_top-3\"]]\n",
        "print(df_summary.sort('rank_query-mean').head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvS9hMGGr2Nq",
        "outputId": "f458c78d-6f41-46f2-fb5d-da075ec833a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 4)\n",
            "┌───────────────────────────────────┬─────────────────┬──────────────┬──────────────┐\n",
            "│ method_name                       ┆ rank_query-mean ┆ num_in_top-1 ┆ num_in_top-3 │\n",
            "│ ---                               ┆ ---             ┆ ---          ┆ ---          │\n",
            "│ str                               ┆ f64             ┆ u32          ┆ u32          │\n",
            "╞═══════════════════════════════════╪═════════════════╪══════════════╪══════════════╡\n",
            "│ multi-qa-mpnet-base-dot-v1_trans… ┆ 39.391304       ┆ 1            ┆ 6            │\n",
            "│ multi-qa-mpnet-base-dot-v1_trans… ┆ 39.927536       ┆ 1            ┆ 5            │\n",
            "│ multi-qa-distilbert-cos-v1_title… ┆ 40.028986       ┆ 1            ┆ 6            │\n",
            "│ multi-qa-distilbert-cos-v1_title… ┆ 40.028986       ┆ 1            ┆ 6            │\n",
            "│ multi-qa-distilbert-cos-v1_title… ┆ 40.043478       ┆ 1            ┆ 6            │\n",
            "└───────────────────────────────────┴─────────────────┴──────────────┴──────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary.sort('rank_query-mean').head()[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pucmxrHSr_sl",
        "outputId": "6cd618e2-1b4c-4b87-d990-3bc588ee26ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'multi-qa-mpnet-base-dot-v1_transcript_dot-score'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_summary.sort(\"num_in_top-1\", descending=True).head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFPC8RX3sCk9",
        "outputId": "9f81e13e-5fa2-4b41-c3fa-cb429f39e159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 4)\n",
            "┌───────────────────────────────────┬─────────────────┬──────────────┬──────────────┐\n",
            "│ method_name                       ┆ rank_query-mean ┆ num_in_top-1 ┆ num_in_top-3 │\n",
            "│ ---                               ┆ ---             ┆ ---          ┆ ---          │\n",
            "│ str                               ┆ f64             ┆ u32          ┆ u32          │\n",
            "╞═══════════════════════════════════╪═════════════════╪══════════════╪══════════════╡\n",
            "│ all-MiniLM-L6-v2_transcript_cheb… ┆ 40.275362       ┆ 3            ┆ 8            │\n",
            "│ multi-qa-distilbert-cos-v1_trans… ┆ 43.304348       ┆ 3            ┆ 6            │\n",
            "│ multi-qa-mpnet-base-dot-v1_trans… ┆ 49.173913       ┆ 3            ┆ 4            │\n",
            "│ all-MiniLM-L6-v2_transcript_eucl… ┆ 40.637681       ┆ 2            ┆ 6            │\n",
            "│ all-MiniLM-L6-v2_transcript_manh… ┆ 40.84058        ┆ 2            ┆ 6            │\n",
            "└───────────────────────────────────┴─────────────────┴──────────────┴──────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary.sort(\"num_in_top-1\", descending=True).head()[0,0]\n",
        "print(df_summary.sort(\"num_in_top-3\", descending=True).head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOp6Mz_KsEvo",
        "outputId": "462b5521-37fb-4828-99d0-a4cd5f54f104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 4)\n",
            "┌───────────────────────────────────┬─────────────────┬──────────────┬──────────────┐\n",
            "│ method_name                       ┆ rank_query-mean ┆ num_in_top-1 ┆ num_in_top-3 │\n",
            "│ ---                               ┆ ---             ┆ ---          ┆ ---          │\n",
            "│ str                               ┆ f64             ┆ u32          ┆ u32          │\n",
            "╞═══════════════════════════════════╪═════════════════╪══════════════╪══════════════╡\n",
            "│ all-MiniLM-L6-v2_transcript_cheb… ┆ 40.275362       ┆ 3            ┆ 8            │\n",
            "│ multi-qa-mpnet-base-dot-v1_title… ┆ 40.768116       ┆ 1            ┆ 8            │\n",
            "│ multi-qa-mpnet-base-dot-v1_title… ┆ 40.536232       ┆ 1            ┆ 8            │\n",
            "│ multi-qa-mpnet-base-dot-v1_title… ┆ 40.57971        ┆ 1            ┆ 8            │\n",
            "│ all-MiniLM-L6-v2_title_chebyshev  ┆ 45.246377       ┆ 0            ┆ 7            │\n",
            "└───────────────────────────────────┴─────────────────┴──────────────┴──────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary.sort(\"num_in_top-3\", descending=True).head()[0,0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8L1z05DQsKIT",
        "outputId": "29f98143-d1c3-4c80-8a19-52a276cae0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'all-MiniLM-L6-v2_transcript_chebyshev'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    print(df_summary.sort(\"num_in_top-3\", descending=True)['method_name'][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igjq5604sXM0",
        "outputId": "6f9c66d7-94fc-428f-8fc3-9cead9080ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all-MiniLM-L6-v2_transcript_chebyshev\n",
            "multi-qa-mpnet-base-dot-v1_title-transcript_euclidean\n",
            "multi-qa-mpnet-base-dot-v1_title-transcript_manhattan\n",
            "multi-qa-mpnet-base-dot-v1_title-transcript_cos-sim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create video index**"
      ],
      "metadata": {
        "id": "sAdltBGlANes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "embed titles and transcripts"
      ],
      "metadata": {
        "id": "hlIduembAbGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'all-MiniLM-L6-v2'\n",
        "column_name_list = ['title', 'transcript']"
      ],
      "metadata": {
        "id": "x_kh7Yt5Ac-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f217c7a9-f0a4-4bbf-8283-efd7309cd612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b834b97-9b36-42c3-bca6-71daf75f90b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "for column_name in column_name_list:\n",
        "    # generate embeddings\n",
        "    embedding_arr = model.encode(df[column_name].to_list())\n",
        "\n",
        "    # store embeddings in a dataframe\n",
        "    schema_dict = {column_name+'_embedding-'+str(i): float for i in range(embedding_arr.shape[1])}\n",
        "    df_embedding = pl.DataFrame(embedding_arr, schema=schema_dict)\n",
        "\n",
        "    # append embeddings to video index\n",
        "    df = pl.concat([df, df_embedding], how='horizontal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fefe146e-712f-462b-8f7f-d5e0fda0723f",
        "outputId": "d3364162-97c0-47b6-c5b1-8f9a0d25518b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 772)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15b440c8-7b54-4bd6-a132-c04db48aa79d",
        "outputId": "31f83cd7-3d3d-4564-fff0-e10826b6551d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (5, 772)\n",
              "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
              "│ video_id  ┆ datetime  ┆ title     ┆ transcrip ┆ … ┆ transcrip ┆ transcrip ┆ transcrip ┆ transcri │\n",
              "│ ---       ┆ ---       ┆ ---       ┆ t         ┆   ┆ t_embeddi ┆ t_embeddi ┆ t_embeddi ┆ pt_embed │\n",
              "│ str       ┆ datetime[ ┆ str       ┆ ---       ┆   ┆ ng-380    ┆ ng-381    ┆ ng-382    ┆ ding-383 │\n",
              "│           ┆ μs]       ┆           ┆ str       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
              "│           ┆           ┆           ┆           ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
              "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
              "│ 7Oy2NmPwJ ┆ 2024-09-2 ┆ I Quit My ┆ 14 months ┆ … ┆ 0.067269  ┆ -0.09923  ┆ -0.062003 ┆ -0.02224 │\n",
              "│ Xo        ┆ 6         ┆ Job…      ┆ ago I     ┆   ┆           ┆           ┆           ┆ 1        │\n",
              "│           ┆ 23:24:35  ┆ Here’s    ┆ made a    ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ How Much  ┆ big life  ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ I…        ┆ …         ┆   ┆           ┆           ┆           ┆          │\n",
              "│ ZVVkdXHqE ┆ 2024-09-2 ┆ Knowledge ┆ knowledge ┆ … ┆ 0.07171   ┆ 0.048973  ┆ 0.080141  ┆ -0.01256 │\n",
              "│ uM        ┆ 3         ┆ Distillat ┆ distillat ┆   ┆           ┆           ┆           ┆ 1        │\n",
              "│           ┆ 15:45:12  ┆ ion Expla ┆ ion expla ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ ined…     ┆ ined…     ┆   ┆           ┆           ┆           ┆          │\n",
              "│ reXoKNC_W ┆ 2024-09-2 ┆ Quantizat ┆ here's    ┆ … ┆ 0.003317  ┆ -0.027275 ┆ 0.005196  ┆ -0.03153 │\n",
              "│ x4        ┆ 0         ┆ ion       ┆ quantizat ┆   ┆           ┆           ┆           ┆ 2        │\n",
              "│           ┆ 18:15:44  ┆ Explained ┆ ion       ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ in 60     ┆ explained ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ Sec…      ┆ in…       ┆   ┆           ┆           ┆           ┆          │\n",
              "│ 9joIFeKuf ┆ 2024-09-1 ┆ Python    ┆ here's    ┆ … ┆ -0.016807 ┆ 0.155034  ┆ 0.110833  ┆ -0.01281 │\n",
              "│ 04        ┆ 6         ┆ Explained ┆ python    ┆   ┆           ┆           ┆           ┆ 6        │\n",
              "│           ┆ 14:01:44  ┆ in 60     ┆ explained ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ Seconds   ┆ in 60 se… ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ #…        ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "│ pNg2DJ4sp ┆ 2024-09-1 ┆ [Mini-Cou ┆ python is ┆ … ┆ 0.018824  ┆ 0.148439  ┆ 0.044634  ┆ -0.00522 │\n",
              "│ Xg        ┆ 2         ┆ rse]      ┆ the go-to ┆   ┆           ┆           ┆           ┆ 3        │\n",
              "│           ┆ 16:36:12  ┆ Python    ┆ programmi ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ QuickStar ┆ ng …      ┆   ┆           ┆           ┆           ┆          │\n",
              "│           ┆           ┆ t …       ┆           ┆   ┆           ┆           ┆           ┆          │\n",
              "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (5, 772)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>video_id</th><th>datetime</th><th>title</th><th>transcript</th><th>title_embedding-0</th><th>title_embedding-1</th><th>title_embedding-2</th><th>title_embedding-3</th><th>title_embedding-4</th><th>title_embedding-5</th><th>title_embedding-6</th><th>title_embedding-7</th><th>title_embedding-8</th><th>title_embedding-9</th><th>title_embedding-10</th><th>title_embedding-11</th><th>title_embedding-12</th><th>title_embedding-13</th><th>title_embedding-14</th><th>title_embedding-15</th><th>title_embedding-16</th><th>title_embedding-17</th><th>title_embedding-18</th><th>title_embedding-19</th><th>title_embedding-20</th><th>title_embedding-21</th><th>title_embedding-22</th><th>title_embedding-23</th><th>title_embedding-24</th><th>title_embedding-25</th><th>title_embedding-26</th><th>title_embedding-27</th><th>title_embedding-28</th><th>title_embedding-29</th><th>title_embedding-30</th><th>title_embedding-31</th><th>title_embedding-32</th><th>&hellip;</th><th>transcript_embedding-347</th><th>transcript_embedding-348</th><th>transcript_embedding-349</th><th>transcript_embedding-350</th><th>transcript_embedding-351</th><th>transcript_embedding-352</th><th>transcript_embedding-353</th><th>transcript_embedding-354</th><th>transcript_embedding-355</th><th>transcript_embedding-356</th><th>transcript_embedding-357</th><th>transcript_embedding-358</th><th>transcript_embedding-359</th><th>transcript_embedding-360</th><th>transcript_embedding-361</th><th>transcript_embedding-362</th><th>transcript_embedding-363</th><th>transcript_embedding-364</th><th>transcript_embedding-365</th><th>transcript_embedding-366</th><th>transcript_embedding-367</th><th>transcript_embedding-368</th><th>transcript_embedding-369</th><th>transcript_embedding-370</th><th>transcript_embedding-371</th><th>transcript_embedding-372</th><th>transcript_embedding-373</th><th>transcript_embedding-374</th><th>transcript_embedding-375</th><th>transcript_embedding-376</th><th>transcript_embedding-377</th><th>transcript_embedding-378</th><th>transcript_embedding-379</th><th>transcript_embedding-380</th><th>transcript_embedding-381</th><th>transcript_embedding-382</th><th>transcript_embedding-383</th></tr><tr><td>str</td><td>datetime[μs]</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;7Oy2NmPwJXo&quot;</td><td>2024-09-26 23:24:35</td><td>&quot;I Quit My Job……</td><td>&quot;14 months ago …</td><td>0.016464</td><td>0.04102</td><td>0.014321</td><td>0.00988</td><td>0.011357</td><td>-0.010424</td><td>0.020959</td><td>0.021181</td><td>-0.041766</td><td>-0.01471</td><td>-0.035875</td><td>0.003575</td><td>-0.04655</td><td>-0.050529</td><td>-0.111879</td><td>0.025027</td><td>-0.015862</td><td>0.022012</td><td>-0.041443</td><td>-0.077338</td><td>0.036588</td><td>0.011112</td><td>-0.02347</td><td>0.019641</td><td>0.095275</td><td>0.084268</td><td>-0.009453</td><td>0.040754</td><td>-0.002543</td><td>0.006282</td><td>-0.003315</td><td>0.092621</td><td>-0.049112</td><td>&hellip;</td><td>0.052521</td><td>-0.035608</td><td>-0.01663</td><td>-0.017407</td><td>-0.089023</td><td>-0.035869</td><td>-0.022163</td><td>0.021232</td><td>-0.15436</td><td>0.056134</td><td>0.005767</td><td>-0.015907</td><td>0.042088</td><td>0.037927</td><td>0.013388</td><td>0.032578</td><td>0.043277</td><td>-0.056996</td><td>-0.020893</td><td>-0.048742</td><td>-0.020756</td><td>-0.002496</td><td>0.014135</td><td>-0.013788</td><td>-0.026399</td><td>0.024453</td><td>0.014255</td><td>0.061749</td><td>0.018591</td><td>-0.038786</td><td>0.055462</td><td>-0.117372</td><td>0.083418</td><td>0.067269</td><td>-0.09923</td><td>-0.062003</td><td>-0.022241</td></tr><tr><td>&quot;ZVVkdXHqEuM&quot;</td><td>2024-09-23 15:45:12</td><td>&quot;Knowledge Dist…</td><td>&quot;knowledge dist…</td><td>-0.011732</td><td>-0.04271</td><td>0.088132</td><td>0.034729</td><td>-0.012911</td><td>-0.05419</td><td>0.012225</td><td>-0.049659</td><td>-0.000252</td><td>-0.049076</td><td>-0.036361</td><td>0.02852</td><td>-0.012546</td><td>-0.002948</td><td>-0.102184</td><td>0.018863</td><td>-0.028445</td><td>0.060415</td><td>-0.18645</td><td>-0.11647</td><td>0.017884</td><td>-0.026459</td><td>-0.048636</td><td>0.00278</td><td>0.058819</td><td>0.033135</td><td>0.001362</td><td>-0.005529</td><td>0.081662</td><td>-0.060976</td><td>0.055198</td><td>0.03866</td><td>0.04241</td><td>&hellip;</td><td>0.183374</td><td>-0.010083</td><td>0.001357</td><td>-0.065074</td><td>-0.022382</td><td>-0.044083</td><td>-0.017056</td><td>0.027268</td><td>0.079017</td><td>-0.040486</td><td>-0.09567</td><td>-0.023765</td><td>0.000538</td><td>-0.055429</td><td>-0.011123</td><td>-0.087084</td><td>0.04548</td><td>0.001493</td><td>0.052873</td><td>-0.046608</td><td>-0.024501</td><td>0.003573</td><td>0.147117</td><td>0.055381</td><td>0.02479</td><td>0.008054</td><td>-0.078529</td><td>0.037899</td><td>0.025007</td><td>-0.043297</td><td>-0.036289</td><td>-0.054006</td><td>-0.068295</td><td>0.07171</td><td>0.048973</td><td>0.080141</td><td>-0.012561</td></tr><tr><td>&quot;reXoKNC_Wx4&quot;</td><td>2024-09-20 18:15:44</td><td>&quot;Quantization E…</td><td>&quot;here&#x27;s quantiz…</td><td>-0.017366</td><td>0.003107</td><td>0.042879</td><td>-0.018499</td><td>-0.015287</td><td>0.04136</td><td>0.042744</td><td>-0.040049</td><td>0.072668</td><td>-0.031978</td><td>0.045571</td><td>-0.070918</td><td>-0.060834</td><td>-0.000611</td><td>-0.055284</td><td>0.00995</td><td>0.002527</td><td>0.039422</td><td>-0.155163</td><td>-0.07763</td><td>0.075676</td><td>-0.047264</td><td>-0.056142</td><td>-0.031138</td><td>0.084492</td><td>0.086777</td><td>0.002662</td><td>-0.04727</td><td>0.118983</td><td>-0.006494</td><td>0.003519</td><td>0.031334</td><td>0.146434</td><td>&hellip;</td><td>0.048561</td><td>0.005305</td><td>0.007814</td><td>-0.03517</td><td>0.057467</td><td>0.050811</td><td>0.080404</td><td>0.018837</td><td>0.031748</td><td>-0.094081</td><td>-0.069966</td><td>-0.038736</td><td>-0.113055</td><td>-0.057861</td><td>-0.025204</td><td>-0.0637</td><td>0.033459</td><td>0.077462</td><td>-0.004054</td><td>0.002579</td><td>-0.000094</td><td>0.024236</td><td>0.069977</td><td>0.035871</td><td>0.098027</td><td>-0.011177</td><td>-0.028015</td><td>0.043403</td><td>0.087096</td><td>0.007778</td><td>-0.112687</td><td>-0.057943</td><td>0.012512</td><td>0.003317</td><td>-0.027275</td><td>0.005196</td><td>-0.031532</td></tr><tr><td>&quot;9joIFeKuf04&quot;</td><td>2024-09-16 14:01:44</td><td>&quot;Python Explain…</td><td>&quot;here&#x27;s python …</td><td>-0.038469</td><td>0.082332</td><td>-0.036736</td><td>0.032346</td><td>0.006401</td><td>-0.078159</td><td>0.007619</td><td>0.028922</td><td>0.015485</td><td>-0.044059</td><td>0.025412</td><td>0.042849</td><td>0.022507</td><td>-0.012819</td><td>-0.020825</td><td>-0.06091</td><td>-0.062335</td><td>-0.043307</td><td>-0.068869</td><td>-0.123644</td><td>0.030821</td><td>0.005366</td><td>-0.049676</td><td>0.031169</td><td>0.034544</td><td>-0.000553</td><td>-0.023164</td><td>0.006153</td><td>0.061058</td><td>0.000124</td><td>-0.026859</td><td>0.087862</td><td>0.109442</td><td>&hellip;</td><td>0.048234</td><td>-0.019621</td><td>-0.067269</td><td>-0.055684</td><td>-0.083109</td><td>-0.030991</td><td>0.085118</td><td>0.101977</td><td>-0.01814</td><td>-0.021754</td><td>-0.110141</td><td>-0.015989</td><td>0.019048</td><td>-0.004619</td><td>0.002858</td><td>0.012075</td><td>-0.030145</td><td>0.001849</td><td>0.028491</td><td>-0.086656</td><td>-0.058251</td><td>-0.0493</td><td>-0.019976</td><td>0.005962</td><td>-0.034201</td><td>0.007495</td><td>-0.012532</td><td>0.125039</td><td>0.089613</td><td>-0.084917</td><td>-0.03865</td><td>-0.028089</td><td>0.07273</td><td>-0.016807</td><td>0.155034</td><td>0.110833</td><td>-0.012816</td></tr><tr><td>&quot;pNg2DJ4spXg&quot;</td><td>2024-09-12 16:36:12</td><td>&quot;[Mini-Course] …</td><td>&quot;python is the …</td><td>-0.047721</td><td>-0.018628</td><td>-0.005227</td><td>0.022627</td><td>0.035128</td><td>-0.053688</td><td>0.046579</td><td>0.018887</td><td>-0.105469</td><td>-0.03649</td><td>-0.00732</td><td>0.012288</td><td>0.038507</td><td>0.051686</td><td>-0.007194</td><td>-0.010745</td><td>-0.008628</td><td>-0.010041</td><td>0.035027</td><td>-0.105324</td><td>-0.046487</td><td>0.002686</td><td>0.052661</td><td>-0.032843</td><td>0.001482</td><td>0.007868</td><td>0.012435</td><td>-0.005938</td><td>0.021736</td><td>-0.039979</td><td>-0.012299</td><td>0.013308</td><td>0.034286</td><td>&hellip;</td><td>0.038654</td><td>-0.046249</td><td>-0.075403</td><td>0.004189</td><td>-0.056375</td><td>0.018184</td><td>0.004823</td><td>0.061335</td><td>-0.047944</td><td>0.041659</td><td>-0.099861</td><td>-0.013285</td><td>0.020605</td><td>-0.019575</td><td>0.069037</td><td>0.00201</td><td>-0.015447</td><td>0.038637</td><td>0.034687</td><td>-0.087143</td><td>-0.121411</td><td>-0.007735</td><td>0.02079</td><td>-0.009096</td><td>-0.04972</td><td>0.010264</td><td>0.042696</td><td>0.098202</td><td>0.119862</td><td>-0.058075</td><td>-0.016034</td><td>0.020093</td><td>0.076945</td><td>0.018824</td><td>0.148439</td><td>0.044634</td><td>-0.005223</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write_parquet('/content/video-index.parquet')\n"
      ],
      "metadata": {
        "id": "cm7DTnQCCHG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Semantic Search Function**"
      ],
      "metadata": {
        "id": "VjIs8OlsCeSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXEE_ZTdCkHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b637243-4817-4d5a-88dd-8a1d9bc1cfac"
      },
      "source": [
        "### imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d26fd0b-08bf-4fea-80ab-415d8c9d2e13"
      },
      "outputs": [],
      "source": [
        "\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02c8ca0e-f22b-4536-9193-cd0956f5b340"
      },
      "source": [
        "### load data, model, and metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c95c5e2a-f8fb-4e6c-90e2-d2cc1b47e989",
        "outputId": "2002362f-d949-4540-c649-4a9eca729ee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.18 ms, sys: 64 µs, total: 5.25 ms\n",
            "Wall time: 5.16 ms\n"
          ]
        }
      ],
      "source": [
        "%time df = pl.scan_parquet('/content/video-index.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93111b80-9f3c-4964-82ff-a108269cbb8c",
        "outputId": "458767f9-895c-466b-d4b7-7b2076752767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 199 ms, sys: 7.11 ms, total: 206 ms\n",
            "Wall time: 1.05 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = 'all-MiniLM-L6-v2'\n",
        "%time model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3238c069-e03a-4512-9473-549682998103",
        "outputId": "a7c5021e-03f2-45f3-feb7-9fb80689684e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 57 µs, sys: 19 µs, total: 76 µs\n",
            "Wall time: 81.5 µs\n"
          ]
        }
      ],
      "source": [
        "dist_name = 'manhattan'\n",
        "%time dist = DistanceMetric.get_metric(dist_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f2067ee-a0f7-448d-a1e2-eccf97df51f6"
      },
      "source": [
        "### search function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5d4a449-6b08-4487-b57a-57a5609c34ae"
      },
      "outputs": [],
      "source": [
        "def returnSearchResults(query: str, index: pl.lazyframe.frame.LazyFrame) -> np.ndarray:\n",
        "    \"\"\"\n",
        "        Function to return indexes of top search results\n",
        "    \"\"\"\n",
        "\n",
        "    # embed query\n",
        "    query_embedding = model.encode(query).reshape(1, -1)\n",
        "\n",
        "    # compute distances between query and titles/transcripts\n",
        "    dist_arr = dist.pairwise(df.select(df.columns[4:388]).collect(), query_embedding) + dist.pairwise(df.select(df.columns[388:]).collect(), query_embedding)\n",
        "\n",
        "    # search paramaters\n",
        "    threshold = 40 # eye balled threshold for manhatten distance\n",
        "    top_k = 5\n",
        "\n",
        "    # evaluate videos close to query based on threshold\n",
        "    idx_below_threshold = np.argwhere(dist_arr.flatten()<threshold).flatten()\n",
        "    # keep top k closest videos\n",
        "    idx_sorted = np.argsort(dist_arr[idx_below_threshold], axis=0).flatten()\n",
        "\n",
        "    # return indexes of search results\n",
        "    return idx_below_threshold[idx_sorted][:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d793b44-10f3-48c9-9469-3ef0d471cfc2",
        "outputId": "9e775631-d377-451d-c45f-bf2449842c44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (5, 2)\n",
            "┌─────────────┬───────────────────────────────────┐\n",
            "│ video_id    ┆ title                             │\n",
            "│ ---         ┆ ---                               │\n",
            "│ str         ┆ str                               │\n",
            "╞═════════════╪═══════════════════════════════════╡\n",
            "│ ytmK_ErTWss ┆ LLMs EXPLAINED in 60 seconds #ai  │\n",
            "│ ZLbVdvOoTKM ┆ How to Build an LLM from Scratch… │\n",
            "│ 3PIqhdRzhxE ┆ Local LLM Fine-tuning on Mac (M1… │\n",
            "│ Ylz779Op9Pw ┆ How to Improve LLMs with RAG (Ov… │\n",
            "│ tFHeUSJAYbE ┆ A Practical Introduction to Larg… │\n",
            "└─────────────┴───────────────────────────────────┘\n"
          ]
        }
      ],
      "source": [
        "query = \"LLM\"\n",
        "idx_result = returnSearchResults(query, df)\n",
        "\n",
        "print(df.select(['video_id', 'title']).collect()[idx_result])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a666a800-406e-40b8-918a-93fd4d166bb4",
        "outputId": "044dc584-8cdf-423c-c3a9-0f3deb5962b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': ['LLMs EXPLAINED in 60 seconds #ai',\n",
              "  'How to Build an LLM from Scratch | An Overview',\n",
              "  'Local LLM Fine-tuning on Mac (M1 16GB)',\n",
              "  'How to Improve LLMs with RAG (Overview + Python Code)',\n",
              "  'A Practical Introduction to Large Language Models (LLMs)'],\n",
              " 'video_id': ['ytmK_ErTWss',\n",
              "  'ZLbVdvOoTKM',\n",
              "  '3PIqhdRzhxE',\n",
              "  'Ylz779Op9Pw',\n",
              "  'tFHeUSJAYbE']}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "df.select(['title', 'video_id']).collect()[idx_result].to_dict(as_series=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a29f85d7-4f79-48e9-82e8-214ec563dec9"
      },
      "source": [
        "### interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "065e3b14-53b2-41a2-9da2-ae401f7720dd"
      },
      "outputs": [],
      "source": [
        "def pseudoSearchAPI(query: str):\n",
        "\n",
        "    # return top 5 search results\n",
        "    idx_result = returnSearchResults(query, df)\n",
        "    response = df.select(['title', 'video_id']).collect()[idx_result].to_dict(as_series=False)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "5118c9b6-8bcc-41da-a074-b3a539d09448"
      },
      "outputs": [],
      "source": [
        "def formatResultText(title: str, video_id: str):\n",
        "\n",
        "    text = markdown_text = f\"\"\"<br> <br>\n",
        "# {title}<br>\n",
        "\n",
        "🔗 [Video Link](https://youtu.be/{video_id})\"\"\"\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "dd45fec6-4d91-4e98-98f2-d6637dc00442"
      },
      "outputs": [],
      "source": [
        "def formatVideoEmbed(video_id: str):\n",
        "\n",
        "    # other options\n",
        "    # embed = '<iframe width=\"640\" height=\"360\" src=\"https://img.youtube.com/vi/'+ video_id +'/0.jpg\" </iframe>'\n",
        "    # embed = '<a href=\"https://youtu.be/'+ video_id +'\"> <img src=\"https://img.youtube.com/vi/'+ video_id +'/0.jpg\" style=\"width:576;height:324;\"></a>'\n",
        "    # embed = '<a href=\"www.youtube.com/watch?v='+ video_id +'\"> <img src=\"https://img.youtube.com/vi/'+ video_id +'/0.jpg\" style=\"width:576;height:324;\"></a>'\n",
        "\n",
        "    return '<iframe width=\"576\" height=\"324\" src=\"https://www.youtube.com/embed/'+ video_id +'\"></iframe>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caff69c4-eba1-46b0-a406-f456263399bd"
      },
      "outputs": [],
      "source": [
        "def searchResults(query):\n",
        "    # pseudo API call\n",
        "    response = pseudoSearchAPI(query)\n",
        "\n",
        "    # format search results\n",
        "\n",
        "    # initialize list of outputs\n",
        "    output_list = []\n",
        "\n",
        "    # compute number of null search results (out of 5)\n",
        "    num_empty_results = 5-len(response['title'])\n",
        "\n",
        "    # display search results\n",
        "    for i in range(len(response['title'])):\n",
        "        video_id = response['video_id'][i]\n",
        "        title = response['title'][i]\n",
        "\n",
        "        embed = gr.HTML(value = formatVideoEmbed(video_id), visible=True)\n",
        "        text = gr.Markdown(value = formatResultText(title, video_id), visible=True)\n",
        "\n",
        "        output_list.append(embed)\n",
        "        output_list.append(text)\n",
        "\n",
        "    # make null search result slots invisible\n",
        "    for i in range(num_empty_results):\n",
        "\n",
        "        # if no search results display \"No results.\" text\n",
        "        if num_empty_results==5 and i==0:\n",
        "            embed = gr.HTML(visible=False)\n",
        "            text = gr.Markdown(value = \"No results. Try rephrasing your query.\", visible=True)\n",
        "\n",
        "            output_list.append(embed)\n",
        "            output_list.append(text)\n",
        "            continue\n",
        "\n",
        "        embed = gr.HTML(visible=False)\n",
        "        text = gr.Markdown(visible=False)\n",
        "\n",
        "        output_list.append(embed)\n",
        "        output_list.append(text)\n",
        "\n",
        "    return output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b1a323fc-e1a3-4a38-8701-e0809dc59b49",
        "outputId": "4650a626-9bce-4114-eb28-f661158653b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gr' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-469a373e8fba>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# YouTube Search\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
          ]
        }
      ],
      "source": [
        "# demo\n",
        "output_list = []\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# YouTube Search\")\n",
        "\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(placeholder=\"What are you looking for?\", label=\"Query\", scale=3)\n",
        "        btn = gr.Button(\"Search\")\n",
        "        btn.click(fn=searchResults, inputs=inp, outputs=output_list)\n",
        "\n",
        "    for i in range(5):\n",
        "        with gr.Row():\n",
        "            output_list.append(gr.HTML())\n",
        "            output_list.append(gr.Markdown())\n",
        "\n",
        "    inp.submit(fn=searchResults, inputs=inp, outputs=output_list)\n",
        "\n",
        "demo.launch()"
      ]
    }
  ]
}